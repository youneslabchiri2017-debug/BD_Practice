{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc7656d4",
   "metadata": {},
   "source": [
    "# Spark Practical Work\n",
    "\n",
    "Authors:\n",
    " - Ahajjam Ziggaf Kanjaa, Mohammed\n",
    " - Labchiri Boukhalef, Younes\n",
    " - Ramírez Castaño, Víctor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223ecea7",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187cb106",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T19:53:10.375459Z",
     "start_time": "2025-12-11T19:53:10.018559Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (col, sum)\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    SQLTransformer,\n",
    "    StringIndexer, \n",
    "    OneHotEncoder, \n",
    "    VectorAssembler, \n",
    "    StandardScaler\n",
    ")\n",
    "from pyspark.ml.regression import (\n",
    "    DecisionTreeRegressor,\n",
    "    RandomForestRegressor,\n",
    "    GBTRegressor\n",
    ")\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import gc\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"8g\").appName(\"FlightModelPrediction\").getOrCreate()\n",
    "\n",
    "data_path = \"../training_data/flight_data/\"\n",
    "\n",
    "df = spark.read.csv(\n",
    "    data_path,\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    nullValue=\"NA\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67259b23",
   "metadata": {},
   "source": [
    "### Explaratory data analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fdac5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T19:53:11.960721Z",
     "start_time": "2025-12-11T19:53:10.384056Z"
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()\n",
    "\n",
    "cat_cols = [\n",
    "    \"UniqueCarrier\",\n",
    "    \"Origin\",\n",
    "    \"Dest\",\n",
    "    \"TailNum\",\n",
    "    \"CancellationCode\"\n",
    "]\n",
    "\n",
    "# Variable info for numerical variables\n",
    "df.describe().show()\n",
    "\n",
    "# Variable info for categorical variables\n",
    "for c in cat_cols:\n",
    "    print(f\"\\nColumn: {c}\")\n",
    "    df.groupBy(c).count() \\\n",
    "        .orderBy(\"count\", ascending=False) \\\n",
    "        .show(5)\n",
    "\n",
    "# Show null values\n",
    "df.select([\n",
    "    sum(col(c).isNull().cast(\"int\")).alias(c)\n",
    "    for c in df.columns\n",
    "]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754059ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split 80/20\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=89)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f1e2c3",
   "metadata": {},
   "source": [
    "To ensure the process is reproducible, we stored all transformations in a Spark ML Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the pipeline\n",
    "stages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb205e0",
   "metadata": {},
   "source": [
    "### Data filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1c0013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do not control that any of this variables has a null value, so if an instance has a null value, it is deleted\n",
    "sql_logic = \"\"\"\n",
    "SELECT Year,\n",
    "       Month,\n",
    "       DayofMonth,\n",
    "       DayOfWeek,\n",
    "       DepTime,\n",
    "       CRSDepTime,\n",
    "       ArrTime,\n",
    "       CRSArrTime,\n",
    "       UniqueCarrier,\n",
    "       FlightNum,\n",
    "       TailNum,\n",
    "       ActualElapsedTime,\n",
    "       CRSElapsedTime,\n",
    "       AirTime,\n",
    "       ArrDelay,\n",
    "       DepDelay,\n",
    "       Origin,\n",
    "       Dest,\n",
    "       Distance,\n",
    "       TaxiIn,\n",
    "       Cancelled,\n",
    "       CancellationCode,\n",
    "       Diverted,\n",
    "       CarrierDelay,\n",
    "       WeatherDelay,\n",
    "       NASDelay,\n",
    "       SecurityDelay,\n",
    "       LateAircraftDelay,\n",
    "       COALESCE(CAST(TaxiOut AS INT), 0) AS TaxiOut\n",
    "FROM __THIS__\n",
    "WHERE CRSDepTime IS NOT NULL\n",
    "  AND CRSArrTime IS NOT NULL\n",
    "  AND ArrDelay IS NOT NULL\n",
    "  AND Year IS NOT NULL\n",
    "  AND Month IS NOT NULL\n",
    "  AND DayofMonth IS NOT NULL\n",
    "  AND DayOfWeek IS NOT NULL\n",
    "  AND UniqueCarrier IS NOT NULL\n",
    "  AND CRSElapsedTime IS NOT NULL\n",
    "  AND DepDelay IS NOT NULL\n",
    "  AND Origin IS NOT NULL\n",
    "  AND Dest IS NOT NULL\n",
    "  AND Distance IS NOT NULL\n",
    "  AND Cancelled != 1 -- A cancelled flight is not considered a delay, so that it does not give us useful information.\n",
    "\"\"\"\n",
    "sql_clean = SQLTransformer(statement=sql_logic)\n",
    "\n",
    "stages.append(sql_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c1cb16",
   "metadata": {},
   "source": [
    "### Feature engineering and selection\n",
    "The following SQL query fills all the missing values of ``TaxiOut`` with its median, also creates 2 new variables: ``TakeOffTime``, which signals the exact moment the plane takes off from the ground, and ``LandingEstimate``, that show a new approximate hour of landing taking into account when the plan took off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef23d5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the variables that contain information that is unknown at the time the plane takes off\n",
    "sql_logic = \"\"\"\n",
    "SELECT \n",
    "    Year, Month, DayofMonth, DayOfWeek, DepTime, CRSDepTime, CRSArrTime, UniqueCarrier, CRSElapsedTime, ArrDelay, DepDelay, Origin, Dest, Distance,\n",
    "    -- Mediana de TaxiOut\n",
    "    COALESCE(\n",
    "        (SELECT percentile_approx(TaxiOut, 0.5) FROM __THIS__), \n",
    "        0\n",
    "    ) AS TaxiOut,\n",
    "\n",
    "    -- Timestamp base\n",
    "    CASE\n",
    "        WHEN CRSDepTime = 2400 THEN\n",
    "            to_timestamp(concat(Year, lpad(Month, 2, '0'), lpad(DayofMonth, 2, '0'), '0000'), 'yyyyMMddHHmm') \n",
    "            + INTERVAL 1 DAY\n",
    "        ELSE\n",
    "            to_timestamp(concat(Year, lpad(Month, 2, '0'), lpad(DayofMonth, 2, '0'), lpad(CRSDepTime, 4, '0')), 'yyyyMMddHHmm')\n",
    "    END AS CRSDepTimestamp,\n",
    "\n",
    "    -- TakeOffTime (HHmm)\n",
    "    CAST(\n",
    "        date_format(\n",
    "            CRSDepTimestamp\n",
    "            + (CAST(DepDelay AS INT)\n",
    "            + CAST(TaxiOut AS INT)) * INTERVAL 1 MINUTE,\n",
    "            'HHmm'\n",
    "        ) AS INT\n",
    "    ) AS TakeOffTime,\n",
    "\n",
    "    -- LandingEst (HHmm)\n",
    "    CAST(\n",
    "        date_format(\n",
    "            CRSDepTimestamp\n",
    "            + (CAST(DepDelay AS INT)\n",
    "            + CAST(TaxiOut AS INT)\n",
    "            + CAST(CRSElapsedTime AS INT)) * INTERVAL 1 MINUTE,\n",
    "            'HHmm'\n",
    "        ) AS INT\n",
    "    ) AS LandingEst\n",
    "\n",
    "FROM __THIS__\n",
    "\"\"\"\n",
    "\n",
    "sql_trans = SQLTransformer(statement=sql_logic)\n",
    "\n",
    "stages.append(sql_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c1f41c",
   "metadata": {},
   "source": [
    "Due to the ``HHmm`` format being not being very useful for machine learning, we are splitting the variables that follow this format into 2 separate variables, one for the hour and another one for the minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc67e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_split_sql = \"\"\"\n",
    "SELECT\n",
    "    *,\n",
    "    \n",
    "    -- CRSDepTime\n",
    "    CAST(FLOOR(CRSDepTime / 100) AS INT)      AS CRSDepTimeHour,\n",
    "    CAST(CRSDepTime % 100 AS INT)             AS CRSDepTimeMinute,\n",
    "\n",
    "    -- CRSArrTime\n",
    "    CAST(FLOOR(CRSArrTime / 100) AS INT)      AS CRSArrTimeHour,\n",
    "    CAST(CRSArrTime % 100 AS INT)             AS CRSArrTimeMinute,\n",
    "\n",
    "    -- TakeOffTime\n",
    "    CAST(FLOOR(TakeOffTime / 100) AS INT)     AS TakeOffTimeHour,\n",
    "    CAST(TakeOffTime % 100 AS INT)            AS TakeOffTimeMinute,\n",
    "\n",
    "    -- LandingEst\n",
    "    CAST(FLOOR(LandingEst / 100) AS INT)      AS LandingEstHour,\n",
    "    CAST(LandingEst % 100 AS INT)             AS LandingEstMinute,\n",
    "\n",
    "    -- DepTime\n",
    "    CAST(FLOOR(COALESCE(DepTime, CRSDepTime) / 100) AS INT) AS DepTimeHour,\n",
    "    CAST((COALESCE(DepTime, CRSDepTime) % 100) AS INT) AS DepTimeMinute\n",
    "\n",
    "\n",
    "FROM __THIS__\n",
    "\"\"\"\n",
    "\n",
    "time_splitter = SQLTransformer(statement=time_split_sql)\n",
    "\n",
    "stages.append(time_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803f92a8",
   "metadata": {},
   "source": [
    "We drop all the ``HHmm`` variables, as well as the ``CRSDepTimestamp`` since it is just a variable used for creating others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fa2176",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_hhmm_sql = \"\"\"\n",
    "SELECT\n",
    "    * EXCEPT (\n",
    "        CRSDepTime,\n",
    "        CRSArrTime,\n",
    "        TakeOffTime,\n",
    "        LandingEst,\n",
    "        DepTime,\n",
    "        CRSDepTimestamp\n",
    "    )\n",
    "FROM __THIS__\n",
    "\"\"\"\n",
    "\n",
    "drop_hhmm = SQLTransformer(statement=drop_hhmm_sql)\n",
    "stages.append(drop_hhmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4a4bed",
   "metadata": {},
   "source": [
    "### Feature encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a7fe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variables selected\n",
    "cat_cols = [\"UniqueCarrier\", \"Origin\", \"Dest\", \"Month\", \"DayOfWeek\", \"DayofMonth\", \"Year\"]\n",
    "\n",
    "# Numerical variables selected\n",
    "num_cols = [\n",
    "    \"DepDelay\", \"TaxiOut\", \"Distance\", \"CRSElapsedTime\", \n",
    "    \"LandingEstMinute\", \"TakeOffTimeMinute\", \"CRSDepTimeMinute\", \"CRSArrTimeMinute\", \"DepTimeMinute\",\n",
    "    \"LandingEstHour\", \"TakeOffTimeHour\", \"CRSDepTimeHour\", \"CRSArrTimeHour\", \"DepTimeHour\"\n",
    "]\n",
    "\n",
    "\n",
    "# Categorical variables encoding\n",
    "input_cols_ohe = []\n",
    "categorical_stages = [] \n",
    "\n",
    "for c in cat_cols:\n",
    "    indexer = StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\")\n",
    "    \n",
    "    encoder = OneHotEncoder(inputCol=f\"{c}_idx\", outputCol=f\"{c}_ohe\")\n",
    "    \n",
    "    categorical_stages += [indexer, encoder]\n",
    "    input_cols_ohe.append(f\"{c}_ohe\")\n",
    "\n",
    "stages += categorical_stages\n",
    "\n",
    "# Numerical variables processing\n",
    "\n",
    "# Grouping the numerical variables to a vector\n",
    "num_assembler = VectorAssembler(inputCols=num_cols, outputCol=\"num_features_raw\", handleInvalid=\"skip\")\n",
    "stages.append(num_assembler)\n",
    "\n",
    "# Standartizing phase\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"num_features_raw\", \n",
    "    outputCol=\"num_features_scaled\", \n",
    "    withStd=True, \n",
    "    withMean=True\n",
    ")\n",
    "stages.append(scaler)\n",
    "\n",
    "# Group new columns\n",
    "assembler_all_inputs = input_cols_ohe + [\"num_features_scaled\"]\n",
    "\n",
    "assembler_all = VectorAssembler(\n",
    "    inputCols=assembler_all_inputs, \n",
    "    outputCol=\"features\"\n",
    ")\n",
    "stages.append(assembler_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ad5b74",
   "metadata": {},
   "source": [
    "### Models to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c251e966",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T19:53:18.023080Z",
     "start_time": "2025-12-11T19:53:12.054651Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Models\n",
    "dt = DecisionTreeRegressor(labelCol=\"ArrDelay\", featuresCol=\"features\")\n",
    "rf = RandomForestRegressor(labelCol=\"ArrDelay\", featuresCol=\"features\")\n",
    "gbt = GBTRegressor(labelCol=\"ArrDelay\", featuresCol=\"features\")\n",
    "\n",
    "# Use a separate pipeline for each model\n",
    "pipeline_dt = Pipeline(stages=stages + [dt])\n",
    "pipeline_rf = Pipeline(stages=stages + [rf])\n",
    "pipeline_gbt = Pipeline(stages=stages + [gbt])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645faf1a",
   "metadata": {},
   "source": [
    "### Hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496c4ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "paramGrid_dt = (ParamGridBuilder()\n",
    "    .addGrid(dt.maxDepth, [5])    # You can add more values if you have the technical muscle to do it. We don’t have it. RAM is very expensive.\n",
    "    .addGrid(dt.maxBins, [32])    # You can add more values if you have the technical muscle to do it. We don’t have it. RAM is very expensive.\n",
    "    .build())\n",
    "\n",
    "paramGrid_rf = (ParamGridBuilder()\n",
    "    .addGrid(rf.numTrees, [20])   # You can add more values if you have the technical muscle to do it. We don’t have it. RAM is very expensive.\n",
    "    .addGrid(rf.maxDepth, [5])    # You can add more values if you have the technical muscle to do it. We don’t have it. RAM is very expensive.\n",
    "    .build())\n",
    "\n",
    "paramGrid_gbt = (ParamGridBuilder()\n",
    "    .addGrid(gbt.maxIter, [10])   # You can add more values if you have the technical muscle to do it. We don’t have it. RAM is very expensive.\n",
    "    .addGrid(gbt.maxDepth, [3])   # You can add more values if you have the technical muscle to do it. We don’t have it. RAM is very expensive.\n",
    "    .build())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6ceed4",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fc27b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evaluador = RegressionEvaluator(labelCol=\"ArrDelay\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "cv_dt = CrossValidator(\n",
    "    estimator=pipeline_dt, \n",
    "    estimatorParamMaps=paramGrid_dt,\n",
    "    evaluator=evaluador,\n",
    "    numFolds=3,\n",
    "    seed=89,\n",
    "    parallelism=1\n",
    ")\n",
    "\n",
    "cv_rf = CrossValidator(\n",
    "    estimator=pipeline_rf,\n",
    "    estimatorParamMaps=paramGrid_rf,\n",
    "    evaluator=evaluador,\n",
    "    numFolds=3,\n",
    "    seed=89,\n",
    "    parallelism=1\n",
    ")\n",
    "\n",
    "cv_gbt = CrossValidator(\n",
    "    estimator=pipeline_gbt,\n",
    "    estimatorParamMaps=paramGrid_gbt,\n",
    "    evaluator=evaluador,\n",
    "    numFolds=3,\n",
    "    seed=89,\n",
    "    parallelism=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ead558",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b190e246",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T19:53:18.705094Z",
     "start_time": "2025-12-11T19:53:18.032714Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "## Assign cross validation to model\n",
    "modelos_cv = [(\"Decision Tree\", cv_dt), (\"Random Forest\", cv_rf), (\"GBT\", cv_gbt)]\n",
    "resultados = []\n",
    "\n",
    "# Evaluator\n",
    "eval_mae = RegressionEvaluator(labelCol=\"ArrDelay\", metricName=\"mae\")\n",
    "\n",
    "for nombre, cv in modelos_cv:\n",
    "    print(f\"Initiating training for {nombre}...\")\n",
    "    \n",
    "    # Clear cache and start garbage collector to gain memory on the JVM\n",
    "    spark.catalog.clearCache()\n",
    "    gc.collect()\n",
    "\n",
    "    #Training\n",
    "    cv_model = cv.fit(train_data) \n",
    "    \n",
    "    #Testing\n",
    "    predicciones = cv_model.transform(test_data)\n",
    "    \n",
    "    #Evaluate result\n",
    "    rmse = evaluador.evaluate(predicciones) \n",
    "    mae = eval_mae.evaluate(predicciones)   \n",
    "    \n",
    "    # Select best evaluator metric\n",
    "    score = mae if rmse > (2 * mae) else rmse\n",
    "    metrica_usada = \"MAE\" if rmse > (2 * mae) else \"RMSE\"\n",
    "    \n",
    "    print(f\"Results {nombre} -> RMSE: {rmse:.2f}, MAE: {mae:.2f} (Score: {score:.2f} using {metrica_usada})\")\n",
    "    \n",
    "    resultados.append({\n",
    "        \"nombre\": nombre,\n",
    "        \"modelo_fit\": cv_model, \n",
    "        \"rmse\": rmse,\n",
    "        \"mae\": mae,\n",
    "        \"score_final\": score,\n",
    "        \"metrica\": metrica_usada\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53649c4f",
   "metadata": {},
   "source": [
    "### Find and save best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bdf908",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mejor_resultado = min(resultados, key=lambda x: x[\"score_final\"])\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Best model: {mejor_resultado['nombre']}\")\n",
    "print(f\"Criteria: {mejor_resultado['metrica']} of {mejor_resultado['score_final']:.4f}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Retrive better functioning model\n",
    "mejor_modelo_final = mejor_resultado['modelo_fit'].bestModel\n",
    "\n",
    "# Save the model for app\n",
    "path_guardado = \"best_model\"\n",
    "mejor_modelo_final.write().overwrite().save(path_guardado)\n",
    "\n",
    "print(f\"The model '{mejor_resultado['nombre']}' has been saved at folder '{path_guardado}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
