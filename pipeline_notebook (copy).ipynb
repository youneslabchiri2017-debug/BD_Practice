{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc7656d4",
   "metadata": {},
   "source": [
    "# Spark Practical Work\n",
    "\n",
    "Authors:\n",
    " - Ahajjan Ziggaf Kanjaa, Mohammed\n",
    " - Labchiri Boukhalef, Younes\n",
    " - Ramírez Castaño, Víctor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223ecea7",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "187cb106",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T19:53:10.375459Z",
     "start_time": "2025-12-11T19:53:10.018559Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (col, sum)\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    SQLTransformer,\n",
    "    StringIndexer, \n",
    "    OneHotEncoder, \n",
    "    VectorAssembler, \n",
    "    StandardScaler\n",
    ")\n",
    "from pyspark.ml.regression import (\n",
    "    DecisionTreeRegressor,\n",
    "    RandomForestRegressor,\n",
    "    GBTRegressor\n",
    ")\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import gc\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"8g\").appName(\"FlightModelPrediction\").getOrCreate()\n",
    "\n",
    "data_path = \"../training_data/flight_data/1988.csv\"\n",
    "\n",
    "df = spark.read.csv(\n",
    "    data_path,\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    nullValue=\"NA\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67259b23",
   "metadata": {},
   "source": [
    "### Explaratory data analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "57fdac5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T19:53:11.960721Z",
     "start_time": "2025-12-11T19:53:10.384056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: integer (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- ArrTime: integer (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- ActualElapsedTime: integer (nullable = true)\n",
      " |-- CRSElapsedTime: integer (nullable = true)\n",
      " |-- AirTime: string (nullable = true)\n",
      " |-- ArrDelay: integer (nullable = true)\n",
      " |-- DepDelay: integer (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      " |-- TaxiIn: string (nullable = true)\n",
      " |-- TaxiOut: string (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: integer (nullable = true)\n",
      " |-- CarrierDelay: string (nullable = true)\n",
      " |-- WeatherDelay: string (nullable = true)\n",
      " |-- NASDelay: string (nullable = true)\n",
      " |-- SecurityDelay: string (nullable = true)\n",
      " |-- LateAircraftDelay: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------------+-----------------+------------------+------------------+------------------+-----------------+------------------+-------------+-----------------+-------+------------------+------------------+-------+------------------+------------------+-------+-------+------------------+------+-------+--------------------+----------------+--------------------+------------+------------+--------+-------------+-----------------+\n",
      "|summary|   Year|             Month|       DayofMonth|         DayOfWeek|           DepTime|        CRSDepTime|          ArrTime|        CRSArrTime|UniqueCarrier|        FlightNum|TailNum| ActualElapsedTime|    CRSElapsedTime|AirTime|          ArrDelay|          DepDelay| Origin|   Dest|          Distance|TaxiIn|TaxiOut|           Cancelled|CancellationCode|            Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+-------+-------+------------------+-----------------+------------------+------------------+------------------+-----------------+------------------+-------------+-----------------+-------+------------------+------------------+-------+------------------+------------------+-------+-------+------------------+------+-------+--------------------+----------------+--------------------+------------+------------+--------+-------------+-----------------+\n",
      "|  count|5202096|           5202096|          5202096|           5202096|           5151933|           5202096|          5137497|           5202096|      5202096|          5202096|      0|           5137497|           5202096|      0|           5137497|           5151933|5202096|5202096|           5190994|     0|      0|             5202096|               0|             5202096|           0|           0|       0|            0|                0|\n",
      "|   mean| 1988.0| 6.508971383842205|15.75753676979433|3.9543607038393755|1363.7786636588635|1357.0670687353713|1493.591975820132|1493.3827745585625|         NULL|  687.01381308611|   NULL|104.04070698240797|103.98662154639207|   NULL| 6.547350003318737| 6.706767731645578|   NULL|   NULL| 601.5666221151479|  NULL|   NULL|0.009642843961357115|            NULL|0.002775035293466326|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "| stddev|    0.0|3.4452009233809564|8.798634504531314|1.9879310442594094| 475.5134685541383|469.70432659492076|493.7432332460107|483.78903391103125|         NULL|518.6402297291613|   NULL| 61.96058482682738|  61.7384365761797|   NULL|23.325170715950783|21.777144381870592|   NULL|   NULL|501.09997680551515|  NULL|   NULL| 0.09772339206897167|            NULL|0.052605465538779615|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "|    min|   1988|                 1|                1|                 1|                 1|                 1|                1|                 1|           AA|                1|   NULL|              -530|               -52|   NULL|             -1185|             -1000|    ABE|    ABE|                10|  NULL|   NULL|                   0|            NULL|                   0|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "|    max|   1988|                12|               31|                 7|              2400|              2400|             2400|              2400|           WN|             6189|   NULL|              1737|              1560|   NULL|              1394|              1439|    YUM|    YUM|              4983|  NULL|   NULL|                   1|            NULL|                   1|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "+-------+-------+------------------+-----------------+------------------+------------------+------------------+-----------------+------------------+-------------+-----------------+-------+------------------+------------------+-------+------------------+------------------+-------+-------+------------------+------+-------+--------------------+----------------+--------------------+------------+------------+--------+-------------+-----------------+\n",
      "\n",
      "\n",
      "Column: UniqueCarrier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|UniqueCarrier| count|\n",
      "+-------------+------+\n",
      "|           DL|753983|\n",
      "|           AA|694757|\n",
      "|           UA|587144|\n",
      "|           US|494383|\n",
      "|           PI|470957|\n",
      "+-------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Column: Origin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|Origin| count|\n",
      "+------+------+\n",
      "|   ORD|271494|\n",
      "|   ATL|259731|\n",
      "|   DFW|216849|\n",
      "|   LAX|169696|\n",
      "|   DEN|161146|\n",
      "+------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Column: Dest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|Dest| count|\n",
      "+----+------+\n",
      "| ORD|274766|\n",
      "| ATL|260573|\n",
      "| DFW|220266|\n",
      "| LAX|169699|\n",
      "| DEN|163598|\n",
      "+----+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Column: TailNum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|TailNum|  count|\n",
      "+-------+-------+\n",
      "|   NULL|5202096|\n",
      "+-------+-------+\n",
      "\n",
      "\n",
      "Column: CancellationCode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+\n",
      "|CancellationCode|  count|\n",
      "+----------------+-------+\n",
      "|            NULL|5202096|\n",
      "+----------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 285:======>                                                (2 + 14) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+-------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn |TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+-------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|0   |0    |0         |0        |50163  |0         |64599  |0         |0            |0        |5202096|64599            |0             |5202096|64599   |50163   |0     |0   |11102   |5202096|5202096|0        |5202096         |0       |5202096     |5202096     |5202096 |5202096      |5202096          |\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+-------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "\n",
    "cat_cols = [\n",
    "    \"UniqueCarrier\",\n",
    "    \"Origin\",\n",
    "    \"Dest\",\n",
    "    \"TailNum\",\n",
    "    \"CancellationCode\"\n",
    "]\n",
    "\n",
    "# Variable info for numerical variables\n",
    "df.describe().show()\n",
    "\n",
    "# Variable info for categorical variables\n",
    "for c in cat_cols:\n",
    "    print(f\"\\nColumn: {c}\")\n",
    "    df.groupBy(c).count() \\\n",
    "        .orderBy(\"count\", ascending=False) \\\n",
    "        .show(5)\n",
    "\n",
    "# Show null values\n",
    "df.select([\n",
    "    sum(col(c).isNull().cast(\"int\")).alias(c)\n",
    "    for c in df.columns\n",
    "]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "754059ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split 80/20\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the pipeline\n",
    "stages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb205e0",
   "metadata": {},
   "source": [
    "### Data filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1c0013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do not control that any of this variables has a null value, so if an instance has a null value, it is deleted\n",
    "sql_logic = \"\"\"\n",
    "SELECT Year,\n",
    "       Month,\n",
    "       DayofMonth,\n",
    "       DayOfWeek,\n",
    "       DepTime,\n",
    "       CRSDepTime,\n",
    "       ArrTime,\n",
    "       CRSArrTime,\n",
    "       UniqueCarrier,\n",
    "       FlightNum,\n",
    "       TailNum,\n",
    "       ActualElapsedTime,\n",
    "       CRSElapsedTime,\n",
    "       AirTime,\n",
    "       ArrDelay,\n",
    "       DepDelay,\n",
    "       Origin,\n",
    "       Dest,\n",
    "       Distance,\n",
    "       TaxiIn,\n",
    "       Cancelled,\n",
    "       CancellationCode,\n",
    "       Diverted,\n",
    "       CarrierDelay,\n",
    "       WeatherDelay,\n",
    "       NASDelay,\n",
    "       SecurityDelay,\n",
    "       LateAircraftDelay,\n",
    "       COALESCE(CAST(TaxiOut AS INT), 0) AS TaxiOut\n",
    "FROM __THIS__\n",
    "WHERE CRSDepTime IS NOT NULL\n",
    "  AND CRSArrTime IS NOT NULL\n",
    "  AND ArrDelay IS NOT NULL\n",
    "  AND Year IS NOT NULL\n",
    "  AND Month IS NOT NULL\n",
    "  AND DayofMonth IS NOT NULL\n",
    "  AND DayOfWeek IS NOT NULL\n",
    "  AND UniqueCarrier IS NOT NULL\n",
    "  AND CRSElapsedTime IS NOT NULL\n",
    "  AND DepDelay IS NOT NULL\n",
    "  AND Origin IS NOT NULL\n",
    "  AND Dest IS NOT NULL\n",
    "  AND Distance IS NOT NULL\n",
    "  AND Cancelled != 1 -- A cancelled flight is not considered a delay, so that it does not give us useful information.\n",
    "\"\"\"\n",
    "sql_clean = SQLTransformer(statement=sql_logic)\n",
    "\n",
    "stages.append(sql_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c1cb16",
   "metadata": {},
   "source": [
    "### Feature engineering and selection\n",
    "The following SQL query fills all the missing values of ``TaxiOut`` with its median, also creates 2 new variables: ``TakeOffTime``, which signals the exact moment the plane takes off from the ground, and ``LandingEstimate``, that show a new approximate hour of landing taking into account when the plan took off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ef23d5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the variables that contain information that is unknown at the time the plane takes off\n",
    "sql_logic = \"\"\"\n",
    "SELECT \n",
    "    Year, Month, DayofMonth, DayOfWeek, DepTime, CRSDepTime, CRSArrTime, UniqueCarrier, CRSElapsedTime, ArrDelay, DepDelay, Origin, Dest, Distance,\n",
    "    -- Mediana de TaxiOut\n",
    "    COALESCE(\n",
    "        (SELECT percentile_approx(TaxiOut, 0.5) FROM __THIS__), \n",
    "        0\n",
    "    ) AS TaxiOut,\n",
    "\n",
    "    -- Timestamp base\n",
    "    CASE\n",
    "        WHEN CRSDepTime = 2400 THEN\n",
    "            to_timestamp(\n",
    "                concat(\n",
    "                    Year,\n",
    "                    lpad(Month, 2, '0'),\n",
    "                    lpad(DayofMonth + 1, 2, '0'),\n",
    "                    '0000'\n",
    "                ),\n",
    "                'yyyyMMddHHmm'\n",
    "            )\n",
    "        ELSE\n",
    "            to_timestamp(\n",
    "                concat(\n",
    "                    Year,\n",
    "                    lpad(Month, 2, '0'),\n",
    "                    lpad(DayofMonth, 2, '0'),\n",
    "                    lpad(CRSDepTime, 4, '0')\n",
    "                ),\n",
    "                'yyyyMMddHHmm'\n",
    "            )\n",
    "    END AS CRSDepTimestamp,\n",
    "\n",
    "    -- TakeOffTime (HHmm)\n",
    "    CAST(\n",
    "        date_format(\n",
    "            CRSDepTimestamp\n",
    "            + (CAST(DepDelay AS INT)\n",
    "            + CAST(TaxiOut AS INT)) * INTERVAL 1 MINUTE,\n",
    "            'HHmm'\n",
    "        ) AS INT\n",
    "    ) AS TakeOffTime,\n",
    "\n",
    "    -- LandingEst (HHmm)\n",
    "    CAST(\n",
    "        date_format(\n",
    "            CRSDepTimestamp\n",
    "            + (CAST(DepDelay AS INT)\n",
    "            + CAST(TaxiOut AS INT)\n",
    "            + CAST(CRSElapsedTime AS INT)) * INTERVAL 1 MINUTE,\n",
    "            'HHmm'\n",
    "        ) AS INT\n",
    "    ) AS LandingEst\n",
    "\n",
    "FROM __THIS__\n",
    "\"\"\"\n",
    "\n",
    "sql_trans = SQLTransformer(statement=sql_logic)\n",
    "\n",
    "stages.append(sql_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c1f41c",
   "metadata": {},
   "source": [
    "Due to the ``HHmm`` format being not being very useful for machine learning, we are splitting the variables that follow this format into 2 separate variables, one for the hour and another one for the minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ddc67e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_split_sql = \"\"\"\n",
    "SELECT\n",
    "    *,\n",
    "    \n",
    "    -- CRSDepTime\n",
    "    CAST(FLOOR(CRSDepTime / 100) AS INT)      AS CRSDepTimeHour,\n",
    "    CAST(CRSDepTime % 100 AS INT)             AS CRSDepTimeMinute,\n",
    "\n",
    "    -- CRSArrTime\n",
    "    CAST(FLOOR(CRSArrTime / 100) AS INT)      AS CRSArrTimeHour,\n",
    "    CAST(CRSArrTime % 100 AS INT)             AS CRSArrTimeMinute,\n",
    "\n",
    "    -- TakeOffTime\n",
    "    CAST(FLOOR(TakeOffTime / 100) AS INT)     AS TakeOffTimeHour,\n",
    "    CAST(TakeOffTime % 100 AS INT)            AS TakeOffTimeMinute,\n",
    "\n",
    "    -- LandingEst\n",
    "    CAST(FLOOR(LandingEst / 100) AS INT)      AS LandingEstHour,\n",
    "    CAST(LandingEst % 100 AS INT)             AS LandingEstMinute,\n",
    "\n",
    "    -- DepTime\n",
    "    CAST(FLOOR(COALESCE(DepTime, CRSDepTime) / 100) AS INT) AS DepTimeHour,\n",
    "    CAST((COALESCE(DepTime, CRSDepTime) % 100) AS INT) AS DepTimeMinute\n",
    "\n",
    "\n",
    "FROM __THIS__\n",
    "\"\"\"\n",
    "\n",
    "time_splitter = SQLTransformer(statement=time_split_sql)\n",
    "\n",
    "stages.append(time_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803f92a8",
   "metadata": {},
   "source": [
    "We drop all the ``HHmm`` variables, as well as the ``CRSDepTimestamp`` since it is just a variable used for creating others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "13fa2176",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_hhmm_sql = \"\"\"\n",
    "SELECT\n",
    "    * EXCEPT (\n",
    "        CRSDepTime,\n",
    "        CRSArrTime,\n",
    "        TakeOffTime,\n",
    "        LandingEst,\n",
    "        DepTime,\n",
    "        CRSDepTimestamp\n",
    "    )\n",
    "FROM __THIS__\n",
    "\"\"\"\n",
    "\n",
    "drop_hhmm = SQLTransformer(statement=drop_hhmm_sql)\n",
    "stages.append(drop_hhmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "42a7fe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variables selected\n",
    "cat_cols = [\"UniqueCarrier\", \"Origin\", \"Dest\", \"Month\", \"DayOfWeek\", \"DayofMonth\", \"Year\"]\n",
    "\n",
    "# Numerical variables selected\n",
    "num_cols = [\n",
    "    \"DepDelay\", \"TaxiOut\", \"Distance\", \"CRSElapsedTime\", \n",
    "    \"LandingEstMinute\", \"TakeOffTimeMinute\", \"CRSDepTimeMinute\", \"CRSArrTimeMinute\", \"DepTimeMinute\",\n",
    "    \"LandingEstHour\", \"TakeOffTimeHour\", \"CRSDepTimeHour\", \"CRSArrTimeHour\", \"DepTimeHour\"\n",
    "]\n",
    "\n",
    "\n",
    "# Categorical variables encoding\n",
    "input_cols_ohe = []\n",
    "categorical_stages = [] \n",
    "\n",
    "for c in cat_cols:\n",
    "    indexer = StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\")\n",
    "    \n",
    "    encoder = OneHotEncoder(inputCol=f\"{c}_idx\", outputCol=f\"{c}_ohe\")\n",
    "    \n",
    "    categorical_stages += [indexer, encoder]\n",
    "    input_cols_ohe.append(f\"{c}_ohe\")\n",
    "\n",
    "stages += categorical_stages\n",
    "\n",
    "# Numerical variables processing\n",
    "\n",
    "# Grouping the numerical variables to a vector\n",
    "num_assembler = VectorAssembler(inputCols=num_cols, outputCol=\"num_features_raw\", handleInvalid=\"skip\")\n",
    "stages.append(num_assembler)\n",
    "\n",
    "# Standartizing phase\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"num_features_raw\", \n",
    "    outputCol=\"num_features_scaled\", \n",
    "    withStd=True, \n",
    "    withMean=True\n",
    ")\n",
    "stages.append(scaler)\n",
    "\n",
    "# Group new columns\n",
    "assembler_all_inputs = input_cols_ohe + [\"num_features_scaled\"]\n",
    "\n",
    "assembler_all = VectorAssembler(\n",
    "    inputCols=assembler_all_inputs, \n",
    "    outputCol=\"features\"\n",
    ")\n",
    "stages.append(assembler_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ad5b74",
   "metadata": {},
   "source": [
    "### Models to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c251e966",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T19:53:18.023080Z",
     "start_time": "2025-12-11T19:53:12.054651Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Models\n",
    "dt = DecisionTreeRegressor(labelCol=\"ArrDelay\", featuresCol=\"features\")\n",
    "rf = RandomForestRegressor(labelCol=\"ArrDelay\", featuresCol=\"features\")\n",
    "gbt = GBTRegressor(labelCol=\"ArrDelay\", featuresCol=\"features\")\n",
    "\n",
    "# Use a separate pipeline for each model\n",
    "pipeline_dt = Pipeline(stages=stages + [dt])\n",
    "pipeline_rf = Pipeline(stages=stages + [rf])\n",
    "pipeline_gbt = Pipeline(stages=stages + [gbt])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645faf1a",
   "metadata": {},
   "source": [
    "### Hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "496c4ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "paramGrid_dt = (ParamGridBuilder()\n",
    "    .addGrid(dt.maxDepth, [5])    # You can add more values if you have the technical muscle to do it. We don’t have it. RAM is very expensive.\n",
    "    .addGrid(dt.maxBins, [32])    # You can add more values if you have the technical muscle to do it. We don’t have it. RAM is very expensive.\n",
    "    .build())\n",
    "\n",
    "paramGrid_rf = (ParamGridBuilder()\n",
    "    .addGrid(rf.numTrees, [20])   # You can add more values if you have the technical muscle to do it. We don’t have it. RAM is very expensive.\n",
    "    .addGrid(rf.maxDepth, [5])    # You can add more values if you have the technical muscle to do it. We don’t have it. RAM is very expensive.\n",
    "    .build())\n",
    "\n",
    "paramGrid_gbt = (ParamGridBuilder()\n",
    "    .addGrid(gbt.maxIter, [10])   # You can add more values if you have the technical muscle to do it. We don’t have it. RAM is very expensive.\n",
    "    .addGrid(gbt.maxDepth, [3])   # You can add more values if you have the technical muscle to do it. We don’t have it. RAM is very expensive.\n",
    "    .build())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6ceed4",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "73fc27b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evaluador = RegressionEvaluator(labelCol=\"ArrDelay\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "cv_dt = CrossValidator(\n",
    "    estimator=pipeline_dt, \n",
    "    estimatorParamMaps=paramGrid_dt,\n",
    "    evaluator=evaluador,\n",
    "    numFolds=3,\n",
    "    seed=89,\n",
    "    parallelism=1\n",
    ")\n",
    "\n",
    "cv_rf = CrossValidator(\n",
    "    estimator=pipeline_rf,\n",
    "    estimatorParamMaps=paramGrid_rf,\n",
    "    evaluator=evaluador,\n",
    "    numFolds=3,\n",
    "    seed=89,\n",
    "    parallelism=1\n",
    ")\n",
    "\n",
    "cv_gbt = CrossValidator(\n",
    "    estimator=pipeline_gbt,\n",
    "    estimatorParamMaps=paramGrid_gbt,\n",
    "    evaluator=evaluador,\n",
    "    numFolds=3,\n",
    "    seed=89,\n",
    "    parallelism=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ead558",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b190e246",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T19:53:18.705094Z",
     "start_time": "2025-12-11T19:53:18.032714Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating training for Decision Tree...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/06 16:27:00 ERROR Executor: Exception in task 0.0 in stage 336.0 (TID 1931)\n",
      "java.lang.IllegalArgumentException: requirement failed: Nothing has been added to this summarizer.\n",
      "\tat scala.Predef$.require(Predef.scala:337)\n",
      "\tat org.apache.spark.ml.stat.SummarizerBuffer.mean(Summarizer.scala:650)\n",
      "\tat org.apache.spark.ml.stat.SummaryBuilderImpl$MetricsAggregate.$anonfun$eval$1(Summarizer.scala:389)\n",
      "\tat scala.collection.immutable.ArraySeq.map(ArraySeq.scala:75)\n",
      "\tat scala.collection.immutable.ArraySeq.map(ArraySeq.scala:35)\n",
      "\tat org.apache.spark.ml.stat.SummaryBuilderImpl$MetricsAggregate.eval(Summarizer.scala:388)\n",
      "\tat org.apache.spark.ml.stat.SummaryBuilderImpl$MetricsAggregate.eval(Summarizer.scala:355)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.eval(interfaces.scala:594)\n",
      "\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$5(AggregationIterator.scala:260)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:100)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:35)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "26/01/06 16:27:01 WARN TaskSetManager: Lost task 0.0 in stage 336.0 (TID 1931) (192.168.1.108 executor driver): java.lang.IllegalArgumentException: requirement failed: Nothing has been added to this summarizer.\n",
      "\tat scala.Predef$.require(Predef.scala:337)\n",
      "\tat org.apache.spark.ml.stat.SummarizerBuffer.mean(Summarizer.scala:650)\n",
      "\tat org.apache.spark.ml.stat.SummaryBuilderImpl$MetricsAggregate.$anonfun$eval$1(Summarizer.scala:389)\n",
      "\tat scala.collection.immutable.ArraySeq.map(ArraySeq.scala:75)\n",
      "\tat scala.collection.immutable.ArraySeq.map(ArraySeq.scala:35)\n",
      "\tat org.apache.spark.ml.stat.SummaryBuilderImpl$MetricsAggregate.eval(Summarizer.scala:388)\n",
      "\tat org.apache.spark.ml.stat.SummaryBuilderImpl$MetricsAggregate.eval(Summarizer.scala:355)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.eval(interfaces.scala:594)\n",
      "\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$5(AggregationIterator.scala:260)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:100)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:35)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "26/01/06 16:27:01 ERROR TaskSetManager: Task 0 in stage 336.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o10611.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 336.0 failed 1 times, most recent failure: Lost task 0.0 in stage 336.0 (TID 1931) (192.168.1.108 executor driver): java.lang.IllegalArgumentException: requirement failed: Nothing has been added to this summarizer.\n\tat scala.Predef$.require(Predef.scala:337)\n\tat org.apache.spark.ml.stat.SummarizerBuffer.mean(Summarizer.scala:650)\n\tat org.apache.spark.ml.stat.SummaryBuilderImpl$MetricsAggregate.$anonfun$eval$1(Summarizer.scala:389)\n\tat scala.collection.immutable.ArraySeq.map(ArraySeq.scala:75)\n\tat scala.collection.immutable.ArraySeq.map(ArraySeq.scala:35)\n\tat org.apache.spark.ml.stat.SummaryBuilderImpl$MetricsAggregate.eval(Summarizer.scala:388)\n\tat org.apache.spark.ml.stat.SummaryBuilderImpl$MetricsAggregate.eval(Summarizer.scala:355)\n\tat org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.eval(interfaces.scala:594)\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$5(AggregationIterator.scala:260)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:100)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:35)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2549)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:462)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:402)\n\tat org.apache.spark.sql.execution.adaptive.ResultQueryStageExec.$anonfun$doMaterialize$1(QueryStageExec.scala:325)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$4(SQLExecution.scala:322)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$3(SQLExecution.scala:320)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:316)\n\tat java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Nothing has been added to this summarizer.\n\tat scala.Predef$.require(Predef.scala:337)\n\tat org.apache.spark.ml.stat.SummarizerBuffer.mean(Summarizer.scala:650)\n\tat org.apache.spark.ml.stat.SummaryBuilderImpl$MetricsAggregate.$anonfun$eval$1(Summarizer.scala:389)\n\tat scala.collection.immutable.ArraySeq.map(ArraySeq.scala:75)\n\tat scala.collection.immutable.ArraySeq.map(ArraySeq.scala:35)\n\tat org.apache.spark.ml.stat.SummaryBuilderImpl$MetricsAggregate.eval(Summarizer.scala:388)\n\tat org.apache.spark.ml.stat.SummaryBuilderImpl$MetricsAggregate.eval(Summarizer.scala:355)\n\tat org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.eval(interfaces.scala:594)\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$5(AggregationIterator.scala:260)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:100)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:35)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\t... 3 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#Training\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m cv_model \u001b[38;5;241m=\u001b[39m \u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#Testing\u001b[39;00m\n\u001b[1;32m     19\u001b[0m predicciones \u001b[38;5;241m=\u001b[39m cv_model\u001b[38;5;241m.\u001b[39mtransform(test_data)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:203\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    208\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/tuning.py:858\u001b[0m, in \u001b[0;36mCrossValidator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    852\u001b[0m train \u001b[38;5;241m=\u001b[39m datasets[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m    854\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    855\u001b[0m     inheritable_thread_target(dataset\u001b[38;5;241m.\u001b[39msparkSession),\n\u001b[1;32m    856\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    857\u001b[0m )\n\u001b[0;32m--> 858\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: f(), tasks):\n\u001b[1;32m    859\u001b[0m     metrics_all[i][j] \u001b[38;5;241m=\u001b[39m metric\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:873\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 873\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    123\u001b[0m job, i, func, args, kwds \u001b[38;5;241m=\u001b[39m task\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrap_exception \u001b[38;5;129;01mand\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _helper_reraises_exception:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/tuning.py:858\u001b[0m, in \u001b[0;36mCrossValidator._fit.<locals>.<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    852\u001b[0m train \u001b[38;5;241m=\u001b[39m datasets[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m    854\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    855\u001b[0m     inheritable_thread_target(dataset\u001b[38;5;241m.\u001b[39msparkSession),\n\u001b[1;32m    856\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    857\u001b[0m )\n\u001b[0;32m--> 858\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, tasks):\n\u001b[1;32m    859\u001b[0m     metrics_all[i][j] \u001b[38;5;241m=\u001b[39m metric\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/util.py:435\u001b[0m, in \u001b[0;36minheritable_thread_target.<locals>.outer.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m tags:\n\u001b[1;32m    434\u001b[0m     session\u001b[38;5;241m.\u001b[39maddTag(tag)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 435\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mff\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/tuning.py:115\u001b[0m, in \u001b[0;36m_parallelFitTasks.<locals>.singleTask\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msingleTask\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, Transformer]:\n\u001b[0;32m--> 115\u001b[0m     index, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodelIter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# TODO: duplicate evaluator to take extra params from input\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m#  Note: Supporting tuning params in evaluator need update method\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;66;03m#  all nested stages and evaluators\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     metric \u001b[38;5;241m=\u001b[39m eva\u001b[38;5;241m.\u001b[39mevaluate(model\u001b[38;5;241m.\u001b[39mtransform(validation, epm[index]))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:96\u001b[0m, in \u001b[0;36m_FitMultipleIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo models remaining.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcounter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfitSingleModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:154\u001b[0m, in \u001b[0;36mEstimator.fitMultiple.<locals>.fitSingleModel\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfitSingleModel\u001b[39m(index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m M:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparamMaps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:201\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(params, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m params:\n\u001b[0;32m--> 201\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/pipeline.py:138\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    136\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:203\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    208\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/util.py:164\u001b[0m, in \u001b[0;36mtry_remote_fit.<locals>.wrapped\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:411\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;129m@try_remote_fit\u001b[39m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 411\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:407\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    284\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o10611.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 336.0 failed 1 times, most recent failure: Lost task 0.0 in stage 336.0 (TID 1931) (192.168.1.108 executor driver): java.lang.IllegalArgumentException: requirement failed: Nothing has been added to this summarizer.\n\tat scala.Predef$.require(Predef.scala:337)\n\tat org.apache.spark.ml.stat.SummarizerBuffer.mean(Summarizer.scala:650)\n\tat org.apache.spark.ml.stat.SummaryBuilderImpl$MetricsAggregate.$anonfun$eval$1(Summarizer.scala:389)\n\tat scala.collection.immutable.ArraySeq.map(ArraySeq.scala:75)\n\tat scala.collection.immutable.ArraySeq.map(ArraySeq.scala:35)\n\tat org.apache.spark.ml.stat.SummaryBuilderImpl$MetricsAggregate.eval(Summarizer.scala:388)\n\tat org.apache.spark.ml.stat.SummaryBuilderImpl$MetricsAggregate.eval(Summarizer.scala:355)\n\tat org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.eval(interfaces.scala:594)\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$5(AggregationIterator.scala:260)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:100)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:35)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2549)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:462)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:402)\n\tat org.apache.spark.sql.execution.adaptive.ResultQueryStageExec.$anonfun$doMaterialize$1(QueryStageExec.scala:325)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$4(SQLExecution.scala:322)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$3(SQLExecution.scala:320)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:316)\n\tat java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Nothing has been added to this summarizer.\n\tat scala.Predef$.require(Predef.scala:337)\n\tat org.apache.spark.ml.stat.SummarizerBuffer.mean(Summarizer.scala:650)\n\tat org.apache.spark.ml.stat.SummaryBuilderImpl$MetricsAggregate.$anonfun$eval$1(Summarizer.scala:389)\n\tat scala.collection.immutable.ArraySeq.map(ArraySeq.scala:75)\n\tat scala.collection.immutable.ArraySeq.map(ArraySeq.scala:35)\n\tat org.apache.spark.ml.stat.SummaryBuilderImpl$MetricsAggregate.eval(Summarizer.scala:388)\n\tat org.apache.spark.ml.stat.SummaryBuilderImpl$MetricsAggregate.eval(Summarizer.scala:355)\n\tat org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.eval(interfaces.scala:594)\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$5(AggregationIterator.scala:260)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:100)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:35)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\t... 3 more\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Assign cross validation to model\n",
    "modelos_cv = [(\"Decision Tree\", cv_dt), (\"Random Forest\", cv_rf), (\"GBT\", cv_gbt)]\n",
    "resultados = []\n",
    "\n",
    "# Evaluator\n",
    "eval_mae = RegressionEvaluator(labelCol=\"ArrDelay\", metricName=\"mae\")\n",
    "\n",
    "for nombre, cv in modelos_cv:\n",
    "    print(f\"Initiating training for {nombre}...\")\n",
    "    \n",
    "    # Clear cache and start garbage collector to gain memory on the JVM\n",
    "    spark.catalog.clearCache()\n",
    "    gc.collect()\n",
    "\n",
    "    #Training\n",
    "    cv_model = cv.fit(train_data) \n",
    "    \n",
    "    #Testing\n",
    "    predicciones = cv_model.transform(test_data)\n",
    "    \n",
    "    #Evaluate result\n",
    "    rmse = evaluador.evaluate(predicciones) \n",
    "    mae = eval_mae.evaluate(predicciones)   \n",
    "    \n",
    "    # Select best evaluator metric\n",
    "    score = mae if rmse > (2 * mae) else rmse\n",
    "    metrica_usada = \"MAE\" if rmse > (2 * mae) else \"RMSE\"\n",
    "    \n",
    "    print(f\"Results {nombre} -> RMSE: {rmse:.2f}, MAE: {mae:.2f} (Score: {score:.2f} using {metrica_usada})\")\n",
    "    \n",
    "    resultados.append({\n",
    "        \"nombre\": nombre,\n",
    "        \"modelo_fit\": cv_model, \n",
    "        \"rmse\": rmse,\n",
    "        \"mae\": mae,\n",
    "        \"score_final\": score,\n",
    "        \"metrica\": metrica_usada\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53649c4f",
   "metadata": {},
   "source": [
    "### Find and save best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bdf908",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mejor_resultado = min(resultados, key=lambda x: x[\"score_final\"])\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Best model: {mejor_resultado['nombre']}\")\n",
    "print(f\"Criteria: {mejor_resultado['metrica']} of {mejor_resultado['score_final']:.4f}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Retrive better functioning model\n",
    "mejor_modelo_final = mejor_resultado['modelo_fit'].bestModel\n",
    "\n",
    "# Save the model for app\n",
    "path_guardado = \"best_model\"\n",
    "mejor_modelo_final.write().overwrite().save(path_guardado)\n",
    "\n",
    "print(f\"The model '{mejor_resultado['nombre']}' has been saved at folder '{path_guardado}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
