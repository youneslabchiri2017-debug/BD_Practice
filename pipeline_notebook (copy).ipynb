{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc7656d4",
   "metadata": {},
   "source": [
    "# Spark Practical Work\n",
    "\n",
    "Authors:\n",
    " - Ahajjan Ziggaf Kanjaa, Mohammed\n",
    " - Labchiri Boukhalef, Younes\n",
    " - Ramírez Castaño, Víctor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223ecea7",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "187cb106",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T19:53:10.375459Z",
     "start_time": "2025-12-11T19:53:10.018559Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (col, sum)\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    SQLTransformer,\n",
    "    Imputer,\n",
    "    StringIndexer, \n",
    "    OneHotEncoder, \n",
    "    VectorAssembler, \n",
    "    StandardScaler\n",
    ")\n",
    "from pyspark.ml.regression import (\n",
    "    DecisionTreeRegressor,\n",
    "    RandomForestRegressor,\n",
    "    GBTRegressor\n",
    ")\n",
    "\n",
    "spark = SparkSession.builder.appName(\"FlightModelPrediction\").getOrCreate()\n",
    "\n",
    "data_path = \"../training_data/flight_data/1988.csv\"\n",
    "\n",
    "df = spark.read.csv(\n",
    "    data_path,\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    nullValue=\"NA\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67259b23",
   "metadata": {},
   "source": [
    "### Explaratory data analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "57fdac5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T19:53:11.960721Z",
     "start_time": "2025-12-11T19:53:10.384056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: integer (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- ArrTime: integer (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- ActualElapsedTime: integer (nullable = true)\n",
      " |-- CRSElapsedTime: integer (nullable = true)\n",
      " |-- AirTime: string (nullable = true)\n",
      " |-- ArrDelay: integer (nullable = true)\n",
      " |-- DepDelay: integer (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      " |-- TaxiIn: string (nullable = true)\n",
      " |-- TaxiOut: string (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: integer (nullable = true)\n",
      " |-- CarrierDelay: string (nullable = true)\n",
      " |-- WeatherDelay: string (nullable = true)\n",
      " |-- NASDelay: string (nullable = true)\n",
      " |-- SecurityDelay: string (nullable = true)\n",
      " |-- LateAircraftDelay: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------------+-----------------+------------------+------------------+------------------+-----------------+------------------+-------------+-----------------+-------+------------------+------------------+-------+------------------+------------------+-------+-------+------------------+------+-------+--------------------+----------------+--------------------+------------+------------+--------+-------------+-----------------+\n",
      "|summary|   Year|             Month|       DayofMonth|         DayOfWeek|           DepTime|        CRSDepTime|          ArrTime|        CRSArrTime|UniqueCarrier|        FlightNum|TailNum| ActualElapsedTime|    CRSElapsedTime|AirTime|          ArrDelay|          DepDelay| Origin|   Dest|          Distance|TaxiIn|TaxiOut|           Cancelled|CancellationCode|            Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+-------+-------+------------------+-----------------+------------------+------------------+------------------+-----------------+------------------+-------------+-----------------+-------+------------------+------------------+-------+------------------+------------------+-------+-------+------------------+------+-------+--------------------+----------------+--------------------+------------+------------+--------+-------------+-----------------+\n",
      "|  count|5202096|           5202096|          5202096|           5202096|           5151933|           5202096|          5137497|           5202096|      5202096|          5202096|      0|           5137497|           5202096|      0|           5137497|           5151933|5202096|5202096|           5190994|     0|      0|             5202096|               0|             5202096|           0|           0|       0|            0|                0|\n",
      "|   mean| 1988.0| 6.508971383842205|15.75753676979433|3.9543607038393755|1363.7786636588635|1357.0670687353713|1493.591975820132|1493.3827745585625|         NULL|  687.01381308611|   NULL|104.04070698240797|103.98662154639207|   NULL| 6.547350003318737| 6.706767731645578|   NULL|   NULL| 601.5666221151479|  NULL|   NULL|0.009642843961357115|            NULL|0.002775035293466326|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "| stddev|    0.0|3.4452009233809564|8.798634504531314|1.9879310442594094| 475.5134685541383|469.70432659492076|493.7432332460107|483.78903391103125|         NULL|518.6402297291613|   NULL| 61.96058482682738|  61.7384365761797|   NULL|23.325170715950783|21.777144381870592|   NULL|   NULL|501.09997680551515|  NULL|   NULL| 0.09772339206897167|            NULL|0.052605465538779615|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "|    min|   1988|                 1|                1|                 1|                 1|                 1|                1|                 1|           AA|                1|   NULL|              -530|               -52|   NULL|             -1185|             -1000|    ABE|    ABE|                10|  NULL|   NULL|                   0|            NULL|                   0|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "|    max|   1988|                12|               31|                 7|              2400|              2400|             2400|              2400|           WN|             6189|   NULL|              1737|              1560|   NULL|              1394|              1439|    YUM|    YUM|              4983|  NULL|   NULL|                   1|            NULL|                   1|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "+-------+-------+------------------+-----------------+------------------+------------------+------------------+-----------------+------------------+-------------+-----------------+-------+------------------+------------------+-------+------------------+------------------+-------+-------+------------------+------+-------+--------------------+----------------+--------------------+------------+------------+--------+-------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 665:=============>                                         (4 + 12) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+-------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn |TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+-------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|0   |0    |0         |0        |50163  |0         |64599  |0         |0            |0        |5202096|64599            |0             |5202096|64599   |50163   |0     |0   |11102   |5202096|5202096|0        |5202096         |0       |5202096     |5202096     |5202096 |5202096      |5202096          |\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+-------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Drop the variables that contain information that is unknown at the time the plane takes off\n",
    "\n",
    "columns_to_drop = [\n",
    "    \"ArrTime\", \"ActualElapsedTime\", \"AirTime\", \"TaxiIn\",\n",
    "    \"Diverted\", \"CarrierDelay\", \"WeatherDelay\",\n",
    "    \"NASDelay\", \"SecurityDelay\", \"LateAircraftDelay\"\n",
    "]\n",
    "\n",
    "df.printSchema()\n",
    "df.describe().show()\n",
    "\n",
    "df.select([\n",
    "    sum(col(c).isNull().cast(\"int\")).alias(c)\n",
    "    for c in df.columns\n",
    "]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8f4200",
   "metadata": {},
   "source": [
    "### Feature selection (FSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "754059ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 668:=========================================>             (12 + 4) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+-------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn |TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+-------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|0   |0    |0         |0        |0      |0         |0      |0         |0            |0        |5137497|0                |0             |5137497|0       |0       |0     |0   |10999   |5137497|5137497|0        |5137497         |0       |5137497     |5137497     |5137497 |5137497      |5137497          |\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+-------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#A cancelled flight is not considered a delay, so that it does not give us useful information.\n",
    "df = df.filter(col(\"Cancelled\") != 1)\n",
    "\n",
    "#If we do not have the attributes 'CRSDepTime' or 'CRSArrTime' we delete that instance\n",
    "df = df.dropna(subset=[\"CRSDepTime\", \"CRSArrTime\"])\n",
    "\n",
    "df = df.na.drop(subset=[\"ArrDelay\"])\n",
    "\n",
    "df.select([\n",
    "    sum(col(c).isNull().cast(\"int\")).alias(c)\n",
    "    for c in df.columns\n",
    "]).show(truncate=False)\n",
    "\n",
    "df = df.withColumn(\"TaxiOut\", col(\"TaxiOut\").cast(\"int\"))\n",
    "\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=89)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Construcción del Pipeline ---\n",
    "stages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c1cb16",
   "metadata": {},
   "source": [
    "### New variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ef23d5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_logic = \"\"\"\n",
    "SELECT \n",
    "    Year, Month, DayofMonth, DayOfWeek, DepTime, CRSDepTime, CRSArrTime, UniqueCarrier, CRSElapsedTime, ArrDelay, DepDelay, Origin, Dest, Distance, TaxiOut, \n",
    "    -- 1 y 2. Lógica para TakeOffTime (FinalDepHHMM convertido a INT)\n",
    "    CAST(date_format(\n",
    "        to_timestamp(concat(Year, lpad(Month, 2, '0'), lpad(DayofMonth, 2, '0'), lpad(CRSDepTime, 4, '0')), 'yyyyMMddHHmm') \n",
    "        + (CAST(DepDelay AS INT) + CAST(TaxiOut AS INT)) * INTERVAL 1 MINUTE, \n",
    "    'HHmm') AS INT) as TakeOffTime,\n",
    "    \n",
    "    -- 3. Lógica para LandingEst (FinalArrHHMM convertido a INT)\n",
    "    CAST(date_format(\n",
    "        to_timestamp(concat(Year, lpad(Month, 2, '0'), lpad(DayofMonth, 2, '0'), lpad(CRSDepTime, 4, '0')), 'yyyyMMddHHmm') \n",
    "        + CAST(CRSElapsedTime AS INT) * INTERVAL 1 MINUTE, \n",
    "    'HHmm') AS INT) as LandingEst\n",
    "\n",
    "FROM __THIS__\n",
    "\"\"\"\n",
    "\n",
    "# Creamos el transformador\n",
    "sql_trans = SQLTransformer(statement=sql_logic)\n",
    "\n",
    "stages.append(sql_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3ecdb7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_taxiOut = Imputer(\n",
    "    inputCols=[\"TaxiOut\"], \n",
    "    outputCols=[\"TaxiOut\"], \n",
    "    strategy=\"median\"\n",
    ")\n",
    "\n",
    "taxiOut_final_fix_logic = \"\"\"\n",
    "    SELECT \n",
    "        * EXCEPT (TaxiOut), \n",
    "        COALESCE(TaxiOut, 0) AS TaxiOut \n",
    "    FROM __THIS__\n",
    "\"\"\"\n",
    "sql_taxiOut_final_fix = SQLTransformer(statement=taxiOut_final_fix_logic)\n",
    "\n",
    "stages.append(imputer_taxiOut)\n",
    "stages.append(sql_taxiOut_final_fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "42a7fe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Definición de Variables ---\n",
    "\n",
    "# Selecciona tus 5 variables categóricas (Ejemplos basados en flight data)\n",
    "cat_cols = [\"UniqueCarrier\", \"Origin\", \"Dest\", \"Month\", \"DayOfWeek\"]\n",
    "\n",
    "# Selecciona tus 10 variables numéricas\n",
    "num_cols = [\n",
    "    \"DepDelay\", \"TaxiOut\", \"Distance\", \"CRSElapsedTime\", \n",
    "    \"LandingEst\", \"TakeOffTime\", \"DayofMonth\", \n",
    "    \"Year\", \"CRSDepTime\", \"CRSArrTime\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# A) Procesamiento de Categóricas (StringIndexer + OHE)\n",
    "input_cols_ohe = []\n",
    "\n",
    "for c in cat_cols:\n",
    "    # 1. Indexar: Convierte strings a índices numéricos\n",
    "    indexer = StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\")\n",
    "    \n",
    "    # 2. OHE: Convierte índices a vectores dispersos (sparse vectors)\n",
    "    encoder = OneHotEncoder(inputCol=f\"{c}_idx\", outputCol=f\"{c}_ohe\")\n",
    "    \n",
    "    # Añadimos pasos al pipeline y guardamos el nombre de la columna de salida\n",
    "    stages += [indexer, encoder]\n",
    "    input_cols_ohe.append(f\"{c}_ohe\")\n",
    "\n",
    "# B) Procesamiento de Numéricas (Assembler + StandardScaler)\n",
    "# Nota: StandardScaler en Spark requiere una columna de tipo Vector, no columnas sueltas.\n",
    "\n",
    "# 1. Agrupar todas las numéricas en un solo vector temporal\n",
    "num_assembler = VectorAssembler(inputCols=num_cols, outputCol=\"num_features_raw\")\n",
    "stages.append(num_assembler)\n",
    "\n",
    "# 2. Estandarizar ese vector (Media 0, Desviación Estándar 1)\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"num_features_raw\", \n",
    "    outputCol=\"num_features_scaled\", \n",
    "    withStd=True, \n",
    "    withMean=True\n",
    ")\n",
    "stages.append(scaler)\n",
    "\n",
    "# C) Ensamblaje Final (Unir OHE + Numéricas Escaladas)\n",
    "\n",
    "# Juntamos las columnas OHE y la columna de numéricas ya escaladas\n",
    "assembler_all_inputs = input_cols_ohe + [\"num_features_scaled\"]\n",
    "\n",
    "assembler_all = VectorAssembler(\n",
    "    inputCols=assembler_all_inputs, \n",
    "    outputCol=\"features\" # Esta es la columna estándar para modelos ML en Spark\n",
    ")\n",
    "stages.append(assembler_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ad5b74",
   "metadata": {},
   "source": [
    "### Baseline model training\n",
    "\n",
    "Three baseline models are trained: `LogisticRegression(max_iter=1000)`, `DecisionTreeClassifier()`, and `MLPClassifier(max_iter=500)`. These models are fitted on the feature-selected and scaled training data. Training multiple models at this stage establishes performance benchmarks for comparison and identifies which algorithms are initially more suitable for the dataset before any tuning is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c251e966",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T19:53:18.023080Z",
     "start_time": "2025-12-11T19:53:12.054651Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import DecisionTreeRegressor, RandomForestRegressor, GBTRegressor\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# 1. Definición de Modelos\n",
    "dt = DecisionTreeRegressor(labelCol=\"ArrDelay\", featuresCol=\"features\")\n",
    "rf = RandomForestRegressor(labelCol=\"ArrDelay\", featuresCol=\"features\")\n",
    "gbt = GBTRegressor(labelCol=\"ArrDelay\", featuresCol=\"features\")\n",
    "\n",
    "# Creación de Pipelines (usando concatenación para mayor limpieza)\n",
    "pipeline_dt = Pipeline(stages=stages + [dt])\n",
    "pipeline_rf = Pipeline(stages=stages + [rf])\n",
    "pipeline_gbt = Pipeline(stages=stages + [gbt])\n",
    "\n",
    "# 2. Rejillas de parámetros (ParamGrids)\n",
    "# IMPORTANTE: Los parámetros deben coincidir con la instancia del modelo\n",
    "\n",
    "paramGrid_dt = (ParamGridBuilder()\n",
    "    .addGrid(dt.maxDepth, [5, 10])\n",
    "    .addGrid(dt.maxBins, [32, 64]) # DT no usa regParam, usamos maxBins\n",
    "    .build())\n",
    "\n",
    "paramGrid_rf = (ParamGridBuilder()\n",
    "    .addGrid(rf.numTrees, [20, 50]) # Parámetro específico de RF\n",
    "    .addGrid(rf.maxDepth, [5, 10])\n",
    "    .build())\n",
    "\n",
    "paramGrid_gbt = (ParamGridBuilder()\n",
    "    .addGrid(gbt.maxIter, [10, 20]) # Parámetro específico de GBT (iteraciones)\n",
    "    .addGrid(gbt.maxDepth, [3, 5])\n",
    "    .build())\n",
    "\n",
    "# 3. CrossValidators\n",
    "# Configuramos el evaluador una sola vez para reutilizarlo\n",
    "evaluador = RegressionEvaluator(labelCol=\"ArrDelay\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "cv_dt = CrossValidator(\n",
    "    estimator=pipeline_dt, \n",
    "    estimatorParamMaps=paramGrid_dt,\n",
    "    evaluator=evaluador,\n",
    "    numFolds=3,\n",
    "    seed=89\n",
    ")\n",
    "\n",
    "cv_rf = CrossValidator(\n",
    "    estimator=pipeline_rf,\n",
    "    estimatorParamMaps=paramGrid_rf,\n",
    "    evaluator=evaluador,\n",
    "    numFolds=3,\n",
    "    seed=89\n",
    ")\n",
    "\n",
    "cv_gbt = CrossValidator(\n",
    "    estimator=pipeline_gbt,\n",
    "    estimatorParamMaps=paramGrid_gbt,\n",
    "    evaluator=evaluador,\n",
    "    numFolds=3,\n",
    "    seed=89\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ead558",
   "metadata": {},
   "source": [
    "### Baseline evaluation\n",
    "\n",
    "The `evaluate_full()` function is used to evaluate each model on the test set. It calculates standard metrics including accuracy, precision, recall, and F1-score. It also plots a confusion matrix, the ROC curve with AUC, and the precision-recall curve with AUC-PR. These metrics provide a comprehensive overview of each model's performance and allow for a more nuanced understanding of strengths and weaknesses, especially in cases of class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b190e246",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T19:53:18.705094Z",
     "start_time": "2025-12-11T19:53:18.032714Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Cross-Validation para Decision Tree...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 671:>                                                      (0 + 16) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8615,261s][warning][gc,alloc] Executor task launch worker for task 6.0 in stage 671.0 (TID 4410): Retried waiting for GCLocker too often allocating 131074 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/29 19:23:25 WARN BlockManager: Block rdd_1692_5 could not be removed as it was not found on disk or in memory\n",
      "25/12/29 19:23:25 WARN BlockManager: Block rdd_1692_6 could not be removed as it was not found on disk or in memory\n",
      "25/12/29 19:23:25 ERROR Executor: Exception in task 5.0 in stage 671.0 (TID 4409)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.unsafe.types.UTF8String.fromAddress(UTF8String.java:177)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.getUTF8String(UnsafeRow.java:397)\n",
      "\tat org.apache.spark.sql.execution.columnar.StringColumnStats.gatherStats(ColumnStats.scala:266)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:55)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.org$apache$spark$sql$execution$columnar$compression$CompressibleColumnBuilder$$super$appendFrom(ColumnBuilder.scala:99)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom(CompressibleColumnBuilder.scala:78)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom$(CompressibleColumnBuilder.scala:77)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.appendFrom(ColumnBuilder.scala:99)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:107)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:82)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:294)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:291)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:234)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x00007f4ec4ddaab0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "25/12/29 19:23:25 ERROR Executor: Exception in task 6.0 in stage 671.0 (TID 4410)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:54)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:559)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:121)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:292)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:232)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x00007f4ec4ddaab0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "25/12/29 19:23:25 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[#3598,Executor task launch worker for task 6.0 in stage 671.0 (TID 4410),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:54)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:559)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:121)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:292)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:232)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x00007f4ec4ddaab0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "25/12/29 19:23:25 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[#3608,Executor task launch worker for task 5.0 in stage 671.0 (TID 4409),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.unsafe.types.UTF8String.fromAddress(UTF8String.java:177)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.getUTF8String(UnsafeRow.java:397)\n",
      "\tat org.apache.spark.sql.execution.columnar.StringColumnStats.gatherStats(ColumnStats.scala:266)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:55)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.org$apache$spark$sql$execution$columnar$compression$CompressibleColumnBuilder$$super$appendFrom(ColumnBuilder.scala:99)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom(CompressibleColumnBuilder.scala:78)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom$(CompressibleColumnBuilder.scala:77)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.appendFrom(ColumnBuilder.scala:99)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:107)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:82)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:294)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:291)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:234)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x00007f4ec4ddaab0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "25/12/29 19:23:25 WARN TaskSetManager: Lost task 5.0 in stage 671.0 (TID 4409) (192.168.1.108 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.unsafe.types.UTF8String.fromAddress(UTF8String.java:177)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.getUTF8String(UnsafeRow.java:397)\n",
      "\tat org.apache.spark.sql.execution.columnar.StringColumnStats.gatherStats(ColumnStats.scala:266)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:55)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.org$apache$spark$sql$execution$columnar$compression$CompressibleColumnBuilder$$super$appendFrom(ColumnBuilder.scala:99)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom(CompressibleColumnBuilder.scala:78)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom$(CompressibleColumnBuilder.scala:77)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.appendFrom(ColumnBuilder.scala:99)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:107)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:82)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:294)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:291)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:234)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x00007f4ec4ddaab0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\n",
      "25/12/29 19:23:25 ERROR TaskSetManager: Task 5 in stage 671.0 failed 1 times; aborting job\n",
      "25/12/29 19:23:26 WARN BlockManager: Putting block rdd_1692_11 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/29 19:23:26 WARN BlockManager: Putting block rdd_1692_8 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/29 19:23:26 WARN BlockManager: Putting block rdd_1692_13 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/29 19:23:26 WARN BlockManager: Block rdd_1692_13 could not be removed as it was not found on disk or in memory\n",
      "25/12/29 19:23:26 WARN BlockManager: Block rdd_1692_8 could not be removed as it was not found on disk or in memory\n",
      "25/12/29 19:23:26 WARN BlockManager: Putting block rdd_1692_15 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/29 19:23:26 WARN BlockManager: Block rdd_1692_15 could not be removed as it was not found on disk or in memory\n",
      "25/12/29 19:23:26 WARN BlockManager: Block rdd_1692_11 could not be removed as it was not found on disk or in memory\n",
      "25/12/29 19:23:26 WARN BlockManager: Putting block rdd_1692_0 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/29 19:23:26 WARN BlockManager: Putting block rdd_1692_12 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/29 19:23:26 WARN BlockManager: Block rdd_1692_12 could not be removed as it was not found on disk or in memory\n",
      "25/12/29 19:23:26 WARN BlockManager: Putting block rdd_1692_14 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/29 19:23:26 WARN BlockManager: Block rdd_1692_0 could not be removed as it was not found on disk or in memory\n",
      "25/12/29 19:23:26 WARN BlockManager: Putting block rdd_1692_10 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/29 19:23:26 WARN BlockManager: Block rdd_1692_10 could not be removed as it was not found on disk or in memory\n",
      "25/12/29 19:23:26 WARN BlockManager: Block rdd_1692_14 could not be removed as it was not found on disk or in memory\n",
      "25/12/29 19:23:26 WARN BlockManager: Putting block rdd_1692_3 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/29 19:23:26 WARN BlockManager: Block rdd_1692_3 could not be removed as it was not found on disk or in memory\n",
      "25/12/29 19:23:26 WARN BlockManager: Putting block rdd_1692_2 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/29 19:23:26 WARN BlockManager: Block rdd_1692_2 could not be removed as it was not found on disk or in memory\n",
      "25/12/29 19:23:26 WARN BlockManager: Putting block rdd_1692_1 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/29 19:23:26 WARN BlockManager: Block rdd_1692_1 could not be removed as it was not found on disk or in memory\n",
      "25/12/29 19:23:26 WARN TaskSetManager: Lost task 11.0 in stage 671.0 (TID 4415) (192.168.1.108 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 5 in stage 671.0 failed 1 times, most recent failure: Lost task 5.0 in stage 671.0 (TID 4409) (192.168.1.108 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.unsafe.types.UTF8String.fromAddress(UTF8String.java:177)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.getUTF8String(UnsafeRow.java:397)\n",
      "\tat org.apache.spark.sql.execution.columnar.StringColumnStats.gatherStats(ColumnStats.scala:266)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:55)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.org$apache$spark$sql$execution$columnar$compression$CompressibleColumnBuilder$$super$appendFrom(ColumnBuilder.scala:99)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom(CompressibleColumnBuilder.scala:78)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom$(CompressibleColumnBuilder.scala:77)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.appendFrom(ColumnBuilder.scala:99)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:107)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:82)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:294)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:291)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:234)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x00007f4ec4ddaab0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/12/29 19:23:26 WARN TaskSetManager: Lost task 0.0 in stage 671.0 (TID 4404) (192.168.1.108 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 5 in stage 671.0 failed 1 times, most recent failure: Lost task 5.0 in stage 671.0 (TID 4409) (192.168.1.108 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.unsafe.types.UTF8String.fromAddress(UTF8String.java:177)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.getUTF8String(UnsafeRow.java:397)\n",
      "\tat org.apache.spark.sql.execution.columnar.StringColumnStats.gatherStats(ColumnStats.scala:266)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:55)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.org$apache$spark$sql$execution$columnar$compression$CompressibleColumnBuilder$$super$appendFrom(ColumnBuilder.scala:99)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom(CompressibleColumnBuilder.scala:78)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom$(CompressibleColumnBuilder.scala:77)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.appendFrom(ColumnBuilder.scala:99)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:107)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:82)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:294)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:291)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:234)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x00007f4ec4ddaab0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/12/29 19:23:26 WARN TaskSetManager: Lost task 8.0 in stage 671.0 (TID 4412) (192.168.1.108 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 5 in stage 671.0 failed 1 times, most recent failure: Lost task 5.0 in stage 671.0 (TID 4409) (192.168.1.108 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.unsafe.types.UTF8String.fromAddress(UTF8String.java:177)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.getUTF8String(UnsafeRow.java:397)\n",
      "\tat org.apache.spark.sql.execution.columnar.StringColumnStats.gatherStats(ColumnStats.scala:266)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:55)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.org$apache$spark$sql$execution$columnar$compression$CompressibleColumnBuilder$$super$appendFrom(ColumnBuilder.scala:99)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom(CompressibleColumnBuilder.scala:78)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom$(CompressibleColumnBuilder.scala:77)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.appendFrom(ColumnBuilder.scala:99)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:107)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:82)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:294)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:291)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:234)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x00007f4ec4ddaab0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/12/29 19:23:26 WARN TaskSetManager: Lost task 2.0 in stage 671.0 (TID 4406) (192.168.1.108 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 5 in stage 671.0 failed 1 times, most recent failure: Lost task 5.0 in stage 671.0 (TID 4409) (192.168.1.108 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.unsafe.types.UTF8String.fromAddress(UTF8String.java:177)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.getUTF8String(UnsafeRow.java:397)\n",
      "\tat org.apache.spark.sql.execution.columnar.StringColumnStats.gatherStats(ColumnStats.scala:266)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:55)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.org$apache$spark$sql$execution$columnar$compression$CompressibleColumnBuilder$$super$appendFrom(ColumnBuilder.scala:99)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom(CompressibleColumnBuilder.scala:78)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom$(CompressibleColumnBuilder.scala:77)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.appendFrom(ColumnBuilder.scala:99)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:107)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:82)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:294)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:291)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:234)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x00007f4ec4ddaab0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/12/29 19:23:26 WARN TaskSetManager: Lost task 14.0 in stage 671.0 (TID 4418) (192.168.1.108 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 5 in stage 671.0 failed 1 times, most recent failure: Lost task 5.0 in stage 671.0 (TID 4409) (192.168.1.108 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.unsafe.types.UTF8String.fromAddress(UTF8String.java:177)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.getUTF8String(UnsafeRow.java:397)\n",
      "\tat org.apache.spark.sql.execution.columnar.StringColumnStats.gatherStats(ColumnStats.scala:266)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:55)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.org$apache$spark$sql$execution$columnar$compression$CompressibleColumnBuilder$$super$appendFrom(ColumnBuilder.scala:99)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom(CompressibleColumnBuilder.scala:78)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom$(CompressibleColumnBuilder.scala:77)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.appendFrom(ColumnBuilder.scala:99)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:107)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:82)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:294)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:291)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:234)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x00007f4ec4ddaab0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/12/29 19:23:26 WARN TaskSetManager: Lost task 13.0 in stage 671.0 (TID 4417) (192.168.1.108 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 5 in stage 671.0 failed 1 times, most recent failure: Lost task 5.0 in stage 671.0 (TID 4409) (192.168.1.108 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.unsafe.types.UTF8String.fromAddress(UTF8String.java:177)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.getUTF8String(UnsafeRow.java:397)\n",
      "\tat org.apache.spark.sql.execution.columnar.StringColumnStats.gatherStats(ColumnStats.scala:266)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:55)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.org$apache$spark$sql$execution$columnar$compression$CompressibleColumnBuilder$$super$appendFrom(ColumnBuilder.scala:99)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom(CompressibleColumnBuilder.scala:78)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom$(CompressibleColumnBuilder.scala:77)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.appendFrom(ColumnBuilder.scala:99)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:107)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:82)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:294)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:291)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:234)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x00007f4ec4ddaab0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/12/29 19:23:26 WARN TaskSetManager: Lost task 15.0 in stage 671.0 (TID 4419) (192.168.1.108 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 5 in stage 671.0 failed 1 times, most recent failure: Lost task 5.0 in stage 671.0 (TID 4409) (192.168.1.108 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.unsafe.types.UTF8String.fromAddress(UTF8String.java:177)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.getUTF8String(UnsafeRow.java:397)\n",
      "\tat org.apache.spark.sql.execution.columnar.StringColumnStats.gatherStats(ColumnStats.scala:266)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:55)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.org$apache$spark$sql$execution$columnar$compression$CompressibleColumnBuilder$$super$appendFrom(ColumnBuilder.scala:99)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom(CompressibleColumnBuilder.scala:78)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom$(CompressibleColumnBuilder.scala:77)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.appendFrom(ColumnBuilder.scala:99)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:107)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:82)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:294)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:291)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:234)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x00007f4ec4ddaab0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/12/29 19:23:26 WARN TaskSetManager: Lost task 12.0 in stage 671.0 (TID 4416) (192.168.1.108 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 5 in stage 671.0 failed 1 times, most recent failure: Lost task 5.0 in stage 671.0 (TID 4409) (192.168.1.108 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.unsafe.types.UTF8String.fromAddress(UTF8String.java:177)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.getUTF8String(UnsafeRow.java:397)\n",
      "\tat org.apache.spark.sql.execution.columnar.StringColumnStats.gatherStats(ColumnStats.scala:266)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:55)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.org$apache$spark$sql$execution$columnar$compression$CompressibleColumnBuilder$$super$appendFrom(ColumnBuilder.scala:99)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom(CompressibleColumnBuilder.scala:78)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom$(CompressibleColumnBuilder.scala:77)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.appendFrom(ColumnBuilder.scala:99)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:107)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:82)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:294)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:291)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:234)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x00007f4ec4ddaab0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/12/29 19:23:26 WARN TaskSetManager: Lost task 1.0 in stage 671.0 (TID 4405) (192.168.1.108 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 5 in stage 671.0 failed 1 times, most recent failure: Lost task 5.0 in stage 671.0 (TID 4409) (192.168.1.108 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.unsafe.types.UTF8String.fromAddress(UTF8String.java:177)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.getUTF8String(UnsafeRow.java:397)\n",
      "\tat org.apache.spark.sql.execution.columnar.StringColumnStats.gatherStats(ColumnStats.scala:266)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:55)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.org$apache$spark$sql$execution$columnar$compression$CompressibleColumnBuilder$$super$appendFrom(ColumnBuilder.scala:99)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom(CompressibleColumnBuilder.scala:78)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom$(CompressibleColumnBuilder.scala:77)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.appendFrom(ColumnBuilder.scala:99)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:107)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:82)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:294)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:291)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:234)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x00007f4ec4ddaab0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/12/29 19:23:26 WARN TaskSetManager: Lost task 10.0 in stage 671.0 (TID 4414) (192.168.1.108 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 5 in stage 671.0 failed 1 times, most recent failure: Lost task 5.0 in stage 671.0 (TID 4409) (192.168.1.108 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.unsafe.types.UTF8String.fromAddress(UTF8String.java:177)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.getUTF8String(UnsafeRow.java:397)\n",
      "\tat org.apache.spark.sql.execution.columnar.StringColumnStats.gatherStats(ColumnStats.scala:266)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:55)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.org$apache$spark$sql$execution$columnar$compression$CompressibleColumnBuilder$$super$appendFrom(ColumnBuilder.scala:99)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom(CompressibleColumnBuilder.scala:78)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom$(CompressibleColumnBuilder.scala:77)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.appendFrom(ColumnBuilder.scala:99)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:107)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:82)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:294)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:291)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:234)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x00007f4ec4ddaab0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/12/29 19:23:26 WARN TaskSetManager: Lost task 3.0 in stage 671.0 (TID 4407) (192.168.1.108 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 5 in stage 671.0 failed 1 times, most recent failure: Lost task 5.0 in stage 671.0 (TID 4409) (192.168.1.108 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.unsafe.types.UTF8String.fromAddress(UTF8String.java:177)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.getUTF8String(UnsafeRow.java:397)\n",
      "\tat org.apache.spark.sql.execution.columnar.StringColumnStats.gatherStats(ColumnStats.scala:266)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:55)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.org$apache$spark$sql$execution$columnar$compression$CompressibleColumnBuilder$$super$appendFrom(ColumnBuilder.scala:99)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom(CompressibleColumnBuilder.scala:78)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom$(CompressibleColumnBuilder.scala:77)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.appendFrom(ColumnBuilder.scala:99)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:107)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:82)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:294)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:291)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:234)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x00007f4ec4ddaab0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/12/29 19:23:26 WARN BlockManager: Putting block rdd_1692_4 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/29 19:23:26 WARN BlockManager: Block rdd_1692_4 could not be removed as it was not found on disk or in memory\n",
      "25/12/29 19:23:26 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda/0x00007f4ec551e288@6d4fd1e4 rejected from java.util.concurrent.ThreadPoolExecutor@57b5ca0b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 4417]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2081)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:841)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1376)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:140)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:813)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:786)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:73)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:116)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/12/29 19:23:26 WARN BlockManager: Putting block rdd_1692_7 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/29 19:23:26 WARN BlockManager: Block rdd_1692_7 could not be removed as it was not found on disk or in memory\n",
      "25/12/29 19:23:26 WARN BlockManager: Putting block rdd_1692_9 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/29 19:23:26 WARN BlockManager: Block rdd_1692_9 could not be removed as it was not found on disk or in memory\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o28263.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 671.0 failed 1 times, most recent failure: Lost task 5.0 in stage 671.0 (TID 4409) (192.168.1.108 executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.unsafe.types.UTF8String.fromAddress(UTF8String.java:177)\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.getUTF8String(UnsafeRow.java:397)\n\tat org.apache.spark.sql.execution.columnar.StringColumnStats.gatherStats(ColumnStats.scala:266)\n\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:55)\n\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.org$apache$spark$sql$execution$columnar$compression$CompressibleColumnBuilder$$super$appendFrom(ColumnBuilder.scala:99)\n\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom(CompressibleColumnBuilder.scala:78)\n\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom$(CompressibleColumnBuilder.scala:77)\n\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.appendFrom(ColumnBuilder.scala:99)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:107)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:82)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:294)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:291)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:234)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n\tat org.apache.spark.storage.BlockManager$$Lambda/0x00007f4ec4ddaab0.apply(Unknown Source)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2579)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1211)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1205)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1297)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1264)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1250)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1250)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.multipleApproxQuantiles(StatFunctions.scala:102)\n\tat org.apache.spark.sql.classic.DataFrameStatFunctions.$anonfun$approxQuantile$1(DataFrameStatFunctions.scala:51)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:126)\n\tat org.apache.spark.sql.classic.DataFrameStatFunctions.approxQuantile(DataFrameStatFunctions.scala:51)\n\tat org.apache.spark.ml.feature.Imputer.fit(Imputer.scala:181)\n\tat org.apache.spark.ml.feature.Imputer.fit(Imputer.scala:116)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.unsafe.types.UTF8String.fromAddress(UTF8String.java:177)\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.getUTF8String(UnsafeRow.java:397)\n\tat org.apache.spark.sql.execution.columnar.StringColumnStats.gatherStats(ColumnStats.scala:266)\n\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:55)\n\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.org$apache$spark$sql$execution$columnar$compression$CompressibleColumnBuilder$$super$appendFrom(ColumnBuilder.scala:99)\n\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom(CompressibleColumnBuilder.scala:78)\n\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom$(CompressibleColumnBuilder.scala:77)\n\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.appendFrom(ColumnBuilder.scala:99)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:107)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:82)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:294)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:291)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:234)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n\tat org.apache.spark.storage.BlockManager$$Lambda/0x00007f4ec4ddaab0.apply(Unknown Source)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIniciando Cross-Validation para \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnombre\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 1. AJUSTE DE HIPERPARÁMETROS (Tuning)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Aquí Spark entrena los 'n' folds y selecciona la mejor combinación de la rejilla\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m cv_model \u001b[38;5;241m=\u001b[39m \u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 2. EVALUACIÓN (Validación con datos no vistos)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Usamos el cv_model (que ya contiene el mejor modelo interno) para predecir\u001b[39;00m\n\u001b[1;32m     17\u001b[0m predicciones \u001b[38;5;241m=\u001b[39m cv_model\u001b[38;5;241m.\u001b[39mtransform(test_data)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:203\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    208\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/tuning.py:858\u001b[0m, in \u001b[0;36mCrossValidator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    852\u001b[0m train \u001b[38;5;241m=\u001b[39m datasets[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m    854\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    855\u001b[0m     inheritable_thread_target(dataset\u001b[38;5;241m.\u001b[39msparkSession),\n\u001b[1;32m    856\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    857\u001b[0m )\n\u001b[0;32m--> 858\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: f(), tasks):\n\u001b[1;32m    859\u001b[0m     metrics_all[i][j] \u001b[38;5;241m=\u001b[39m metric\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:873\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 873\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    123\u001b[0m job, i, func, args, kwds \u001b[38;5;241m=\u001b[39m task\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrap_exception \u001b[38;5;129;01mand\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _helper_reraises_exception:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/tuning.py:858\u001b[0m, in \u001b[0;36mCrossValidator._fit.<locals>.<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    852\u001b[0m train \u001b[38;5;241m=\u001b[39m datasets[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m    854\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    855\u001b[0m     inheritable_thread_target(dataset\u001b[38;5;241m.\u001b[39msparkSession),\n\u001b[1;32m    856\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    857\u001b[0m )\n\u001b[0;32m--> 858\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, tasks):\n\u001b[1;32m    859\u001b[0m     metrics_all[i][j] \u001b[38;5;241m=\u001b[39m metric\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/util.py:435\u001b[0m, in \u001b[0;36minheritable_thread_target.<locals>.outer.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m tags:\n\u001b[1;32m    434\u001b[0m     session\u001b[38;5;241m.\u001b[39maddTag(tag)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 435\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mff\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/tuning.py:115\u001b[0m, in \u001b[0;36m_parallelFitTasks.<locals>.singleTask\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msingleTask\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, Transformer]:\n\u001b[0;32m--> 115\u001b[0m     index, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodelIter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# TODO: duplicate evaluator to take extra params from input\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m#  Note: Supporting tuning params in evaluator need update method\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;66;03m#  all nested stages and evaluators\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     metric \u001b[38;5;241m=\u001b[39m eva\u001b[38;5;241m.\u001b[39mevaluate(model\u001b[38;5;241m.\u001b[39mtransform(validation, epm[index]))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:96\u001b[0m, in \u001b[0;36m_FitMultipleIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo models remaining.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcounter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfitSingleModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:154\u001b[0m, in \u001b[0;36mEstimator.fitMultiple.<locals>.fitSingleModel\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfitSingleModel\u001b[39m(index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m M:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparamMaps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:201\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(params, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m params:\n\u001b[0;32m--> 201\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/pipeline.py:138\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    136\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:203\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    208\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/util.py:164\u001b[0m, in \u001b[0;36mtry_remote_fit.<locals>.wrapped\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:411\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;129m@try_remote_fit\u001b[39m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 411\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:407\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    284\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o28263.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 671.0 failed 1 times, most recent failure: Lost task 5.0 in stage 671.0 (TID 4409) (192.168.1.108 executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.unsafe.types.UTF8String.fromAddress(UTF8String.java:177)\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.getUTF8String(UnsafeRow.java:397)\n\tat org.apache.spark.sql.execution.columnar.StringColumnStats.gatherStats(ColumnStats.scala:266)\n\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:55)\n\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.org$apache$spark$sql$execution$columnar$compression$CompressibleColumnBuilder$$super$appendFrom(ColumnBuilder.scala:99)\n\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom(CompressibleColumnBuilder.scala:78)\n\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom$(CompressibleColumnBuilder.scala:77)\n\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.appendFrom(ColumnBuilder.scala:99)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:107)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:82)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:294)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:291)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:234)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n\tat org.apache.spark.storage.BlockManager$$Lambda/0x00007f4ec4ddaab0.apply(Unknown Source)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2579)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1211)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1205)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1297)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1264)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1250)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1250)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.multipleApproxQuantiles(StatFunctions.scala:102)\n\tat org.apache.spark.sql.classic.DataFrameStatFunctions.$anonfun$approxQuantile$1(DataFrameStatFunctions.scala:51)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:126)\n\tat org.apache.spark.sql.classic.DataFrameStatFunctions.approxQuantile(DataFrameStatFunctions.scala:51)\n\tat org.apache.spark.ml.feature.Imputer.fit(Imputer.scala:181)\n\tat org.apache.spark.ml.feature.Imputer.fit(Imputer.scala:116)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.unsafe.types.UTF8String.fromAddress(UTF8String.java:177)\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.getUTF8String(UnsafeRow.java:397)\n\tat org.apache.spark.sql.execution.columnar.StringColumnStats.gatherStats(ColumnStats.scala:266)\n\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:55)\n\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.org$apache$spark$sql$execution$columnar$compression$CompressibleColumnBuilder$$super$appendFrom(ColumnBuilder.scala:99)\n\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom(CompressibleColumnBuilder.scala:78)\n\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.appendFrom$(CompressibleColumnBuilder.scala:77)\n\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.appendFrom(ColumnBuilder.scala:99)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:107)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:82)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:294)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:291)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:234)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n\tat org.apache.spark.storage.BlockManager$$Lambda/0x00007f4ec4ddaab0.apply(Unknown Source)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "modelos_cv = [(\"Decision Tree\", cv_dt), (\"Random Forest\", cv_rf), (\"GBT\", cv_gbt)]\n",
    "resultados = []\n",
    "\n",
    "# Evaluadores\n",
    "eval_mae = RegressionEvaluator(labelCol=\"ArrDelay\", metricName=\"mae\")\n",
    "# 'evaluador' ya lo tienes definido arriba como RMSE\n",
    "\n",
    "for nombre, cv in modelos_cv:\n",
    "    print(f\"Iniciando Cross-Validation para {nombre}...\")\n",
    "    \n",
    "    # 1. AJUSTE DE HIPERPARÁMETROS (Tuning)\n",
    "    # Aquí Spark entrena los 'n' folds y selecciona la mejor combinación de la rejilla\n",
    "    cv_model = cv.fit(train_data) \n",
    "    \n",
    "    # 2. EVALUACIÓN (Validación con datos no vistos)\n",
    "    # Usamos el cv_model (que ya contiene el mejor modelo interno) para predecir\n",
    "    predicciones = cv_model.transform(test_data)\n",
    "    \n",
    "    rmse = evaluador.evaluate(predicciones) # RMSE\n",
    "    mae = eval_mae.evaluate(predicciones)   # MAE\n",
    "    \n",
    "    # Tu lógica de decisión: Si RMSE es muy alto respecto al MAE, priorizar MAE\n",
    "    score = mae if rmse > (2 * mae) else rmse\n",
    "    metrica_usada = \"MAE\" if rmse > (2 * mae) else \"RMSE\"\n",
    "    \n",
    "    print(f\"Resultados {nombre} -> RMSE: {rmse:.2f}, MAE: {mae:.2f} (Score: {score:.2f} vía {metrica_usada})\")\n",
    "    \n",
    "    # Guardamos todo el objeto cv_model para poder extraer el mejor modelo después\n",
    "    resultados.append({\n",
    "        \"nombre\": nombre,\n",
    "        \"modelo_fit\": cv_model, \n",
    "        \"rmse\": rmse,\n",
    "        \"mae\": mae,\n",
    "        \"score_final\": score,\n",
    "        \"metrica\": metrica_usada\n",
    "    })\n",
    "\n",
    "# --- PASO D: ENCONTRAR Y GUARDAR EL MEJOR ---\n",
    "\n",
    "# Encontrar el mejor resultado basado en tu score_final\n",
    "mejor_resultado = min(resultados, key=lambda x: x[\"score_final\"])\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"GANADOR: {mejor_resultado['nombre']}\")\n",
    "print(f\"Criterio: {mejor_resultado['metrica']} de {mejor_resultado['score_final']:.4f}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Extraer el modelo definitivo (el que mejor funcionó en el CV)\n",
    "# Nota: cv_model.bestModel devuelve el PipelineModel con los mejores parámetros\n",
    "mejor_modelo_final = mejor_resultado['modelo_fit'].bestModel\n",
    "\n",
    "# 3. ALMACENAMIENTO DEL MODELO (Requisito del proyecto)\n",
    "path_guardado = \"best_model\"\n",
    "mejor_modelo_final.write().overwrite().save(path_guardado)\n",
    "\n",
    "print(f\"Éxito: El modelo '{mejor_resultado['nombre']}' ha sido guardado en la carpeta '{path_guardado}'\")\n",
    "\n",
    "'''modelos_cv = [(\"Decision Tree\", cv_dt), (\"Random Forest\", cv_rf), (\"GBT\", cv_gbt)]\n",
    "resultados = []\n",
    "\n",
    "eval_mae = RegressionEvaluator(labelCol=\"ArrDelay\", metricName=\"mae\")\n",
    "\n",
    "for nombre, cv in modelos_cv:\n",
    "    print(f\"Entrenando {nombre}...\")\n",
    "    fit_model = cv.fit(train_data) # train_data debe estar definido previamente\n",
    "    predicciones = fit_model.transform(test_data)\n",
    "    \n",
    "    rmse = evaluador.evaluate(predicciones) # RMSE\n",
    "    mae = eval_mae.evaluate(predicciones)   # MAE\n",
    "    \n",
    "    # Tu lógica: Si RMSE > 2*MAE, fijarse en MAE, si no en RMSE\n",
    "    score = mae if rmse > (2 * mae) else rmse\n",
    "    metrica_usada = \"MAE\" if rmse > (2 * mae) else \"RMSE\"\n",
    "    \n",
    "    resultados.append({\n",
    "        \"nombre\": nombre,\n",
    "        \"rmse\": rmse,\n",
    "        \"mae\": mae,\n",
    "        \"score_final\": score,\n",
    "        \"metrica\": metrica_usada\n",
    "    })\n",
    "\n",
    "# Encontrar el mejor\n",
    "mejor_modelo = min(resultados, key=lambda x: x[\"score_final\"])\n",
    "print(f\"\\nEl mejor modelo es {mejor_modelo['nombre']} basado en {mejor_modelo['metrica']}\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ed84ff",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "\n",
    "GridSearchCV is used for hyperparameter optimization for each model: logistic regression (`C`, `penalty`, `solver`), decision tree (`max_depth`, `min_samples_split`), and neural network (`hidden_layer_sizes`, `alpha`). The code searches across multiple parameter combinations with 5-fold cross-validation to select the configuration that maximizes accuracy. This step is essential for improving model generalization and ensuring that the final models perform optimally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f08e961",
   "metadata": {},
   "source": [
    "### Final evaluation\n",
    "\n",
    "After tuning, the code retrieves the best model via `grid.best_estimator_` and evaluates it on the test set with `accuracy_score` and `classification_report` again. This step confirms how much the optimized model improves over the baseline. Comparing results allows the user to quantify performance gains resulting from hyperparameter optimization and feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce1c15d",
   "metadata": {},
   "source": [
    "### Interpretability\n",
    "\n",
    "For interpretability, the logistic regression coefficients (betas) are extracted from `best_lr.coef_` and plotted as a horizontal bar chart to visualize the influence of each feature. The decision tree is visualized using `plot_tree()` to show the full branching structure, feature thresholds, Gini impurity, and leaf outcomes. These visualizations help understand how the models make predictions and identify the most important features driving classification decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f847bc63",
   "metadata": {},
   "source": [
    "### Comparison Before vs After Tuning\n",
    "\n",
    "This section summarizes the key metrics (accuracy, precision, recall, F1-score) for all three models in a pandas DataFrame. It compares baseline and tuned models side by side, allowing a clear overview of the improvements achieved through feature selection and hyperparameter tuning. This comparative analysis helps identify which model benefits the most from tuning and provides actionable insights for model selection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
