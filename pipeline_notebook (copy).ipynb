{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc7656d4",
   "metadata": {},
   "source": [
    "# Spark Practical Work\n",
    "\n",
    "Authors:\n",
    " - Ahajjan Ziggaf Kanjaa, Mohammed\n",
    " - Labchiri Boukhalef, Younes\n",
    " - Ramírez Castaño, Víctor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223ecea7",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "187cb106",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T19:53:10.375459Z",
     "start_time": "2025-12-11T19:53:10.018559Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (col, sum)\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    SQLTransformer,\n",
    "    Imputer,\n",
    "    StringIndexer, \n",
    "    OneHotEncoder, \n",
    "    VectorAssembler, \n",
    "    StandardScaler\n",
    ")\n",
    "from pyspark.ml.regression import (\n",
    "    DecisionTreeRegressor,\n",
    "    RandomForestRegressor,\n",
    "    GBTRegressor\n",
    ")\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"8g\").appName(\"FlightModelPrediction\").getOrCreate()\n",
    "\n",
    "data_path = \"../training_data/flight_data/1988.csv\"\n",
    "\n",
    "df = spark.read.csv(\n",
    "    data_path,\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    nullValue=\"NA\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67259b23",
   "metadata": {},
   "source": [
    "### Explaratory data analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57fdac5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T19:53:11.960721Z",
     "start_time": "2025-12-11T19:53:10.384056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: integer (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- ArrTime: integer (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- ActualElapsedTime: integer (nullable = true)\n",
      " |-- CRSElapsedTime: integer (nullable = true)\n",
      " |-- AirTime: string (nullable = true)\n",
      " |-- ArrDelay: integer (nullable = true)\n",
      " |-- DepDelay: integer (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      " |-- TaxiIn: string (nullable = true)\n",
      " |-- TaxiOut: string (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: integer (nullable = true)\n",
      " |-- CarrierDelay: string (nullable = true)\n",
      " |-- WeatherDelay: string (nullable = true)\n",
      " |-- NASDelay: string (nullable = true)\n",
      " |-- SecurityDelay: string (nullable = true)\n",
      " |-- LateAircraftDelay: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------------+-----------------+------------------+------------------+------------------+-----------------+------------------+-------------+-----------------+-------+------------------+------------------+-------+------------------+------------------+-------+-------+------------------+------+-------+--------------------+----------------+--------------------+------------+------------+--------+-------------+-----------------+\n",
      "|summary|   Year|             Month|       DayofMonth|         DayOfWeek|           DepTime|        CRSDepTime|          ArrTime|        CRSArrTime|UniqueCarrier|        FlightNum|TailNum| ActualElapsedTime|    CRSElapsedTime|AirTime|          ArrDelay|          DepDelay| Origin|   Dest|          Distance|TaxiIn|TaxiOut|           Cancelled|CancellationCode|            Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+-------+-------+------------------+-----------------+------------------+------------------+------------------+-----------------+------------------+-------------+-----------------+-------+------------------+------------------+-------+------------------+------------------+-------+-------+------------------+------+-------+--------------------+----------------+--------------------+------------+------------+--------+-------------+-----------------+\n",
      "|  count|5202096|           5202096|          5202096|           5202096|           5151933|           5202096|          5137497|           5202096|      5202096|          5202096|      0|           5137497|           5202096|      0|           5137497|           5151933|5202096|5202096|           5190994|     0|      0|             5202096|               0|             5202096|           0|           0|       0|            0|                0|\n",
      "|   mean| 1988.0| 6.508971383842205|15.75753676979433|3.9543607038393755|1363.7786636588635|1357.0670687353713|1493.591975820132|1493.3827745585625|         NULL|  687.01381308611|   NULL|104.04070698240797|103.98662154639207|   NULL| 6.547350003318737| 6.706767731645578|   NULL|   NULL| 601.5666221151479|  NULL|   NULL|0.009642843961357115|            NULL|0.002775035293466326|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "| stddev|    0.0|3.4452009233809564|8.798634504531314|1.9879310442594094| 475.5134685541383|469.70432659492076|493.7432332460107|483.78903391103125|         NULL|518.6402297291613|   NULL| 61.96058482682738|  61.7384365761797|   NULL|23.325170715950783|21.777144381870592|   NULL|   NULL|501.09997680551515|  NULL|   NULL| 0.09772339206897167|            NULL|0.052605465538779615|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "|    min|   1988|                 1|                1|                 1|                 1|                 1|                1|                 1|           AA|                1|   NULL|              -530|               -52|   NULL|             -1185|             -1000|    ABE|    ABE|                10|  NULL|   NULL|                   0|            NULL|                   0|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "|    max|   1988|                12|               31|                 7|              2400|              2400|             2400|              2400|           WN|             6189|   NULL|              1737|              1560|   NULL|              1394|              1439|    YUM|    YUM|              4983|  NULL|   NULL|                   1|            NULL|                   1|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "+-------+-------+------------------+-----------------+------------------+------------------+------------------+-----------------+------------------+-------------+-----------------+-------+------------------+------------------+-------+------------------+------------------+-------+-------+------------------+------+-------+--------------------+----------------+--------------------+------------+------------+--------+-------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:========================>                                (7 + 9) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+-------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn |TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+-------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|0   |0    |0         |0        |50163  |0         |64599  |0         |0            |0        |5202096|64599            |0             |5202096|64599   |50163   |0     |0   |11102   |5202096|5202096|0        |5202096         |0       |5202096     |5202096     |5202096 |5202096      |5202096          |\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+-------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Drop the variables that contain information that is unknown at the time the plane takes off\n",
    "\n",
    "columns_to_drop = [\n",
    "    \"ArrTime\", \"ActualElapsedTime\", \"AirTime\", \"TaxiIn\",\n",
    "    \"Diverted\", \"CarrierDelay\", \"WeatherDelay\",\n",
    "    \"NASDelay\", \"SecurityDelay\", \"LateAircraftDelay\"\n",
    "]\n",
    "\n",
    "df.printSchema()\n",
    "df.describe().show()\n",
    "\n",
    "df.select([\n",
    "    sum(col(c).isNull().cast(\"int\")).alias(c)\n",
    "    for c in df.columns\n",
    "]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8f4200",
   "metadata": {},
   "source": [
    "### Feature selection (FSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "754059ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:=====================>                                  (6 + 10) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+-------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn |TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+-------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|0   |0    |0         |0        |0      |0         |0      |0         |0            |0        |5137497|0                |0             |5137497|0       |0       |0     |0   |10999   |5137497|5137497|0        |5137497         |0       |5137497     |5137497     |5137497 |5137497      |5137497          |\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+-------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#A cancelled flight is not considered a delay, so that it does not give us useful information.\n",
    "df = df.filter(col(\"Cancelled\") != 1)\n",
    "\n",
    "#If we do not have the attributes 'CRSDepTime' or 'CRSArrTime' we delete that instance\n",
    "df = df.dropna(subset=[\"CRSDepTime\", \"CRSArrTime\"])\n",
    "\n",
    "df = df.na.drop(subset=[\"ArrDelay\"])\n",
    "\n",
    "df.select([\n",
    "    sum(col(c).isNull().cast(\"int\")).alias(c)\n",
    "    for c in df.columns\n",
    "]).show(truncate=False)\n",
    "\n",
    "df = df.withColumn(\"TaxiOut\", col(\"TaxiOut\").cast(\"int\"))\n",
    "\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=89)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Construcción del Pipeline ---\n",
    "stages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c1cb16",
   "metadata": {},
   "source": [
    "### New variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef23d5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_logic = \"\"\"\n",
    "SELECT \n",
    "    Year, Month, DayofMonth, DayOfWeek, DepTime, CRSDepTime, CRSArrTime, UniqueCarrier, CRSElapsedTime, ArrDelay, DepDelay, Origin, Dest, Distance, TaxiOut,\n",
    "    -- 1. Mediana de TaxiOut (robusta)\n",
    "    COALESCE(\n",
    "        (SELECT percentile_approx(TaxiOut, 0.5) FROM __THIS__), \n",
    "        0\n",
    "    ) AS TaxiOut,\n",
    "\n",
    "    -- 2. Timestamp base CORREGIDO (maneja CRSDepTime = 2400)\n",
    "    CASE\n",
    "        WHEN CRSDepTime = 2400 THEN\n",
    "            to_timestamp(\n",
    "                concat(\n",
    "                    Year,\n",
    "                    lpad(Month, 2, '0'),\n",
    "                    lpad(DayofMonth + 1, 2, '0'),\n",
    "                    '0000'\n",
    "                ),\n",
    "                'yyyyMMddHHmm'\n",
    "            )\n",
    "        ELSE\n",
    "            to_timestamp(\n",
    "                concat(\n",
    "                    Year,\n",
    "                    lpad(Month, 2, '0'),\n",
    "                    lpad(DayofMonth, 2, '0'),\n",
    "                    lpad(CRSDepTime, 4, '0')\n",
    "                ),\n",
    "                'yyyyMMddHHmm'\n",
    "            )\n",
    "    END AS CRSDepTimestamp,\n",
    "\n",
    "    -- 3. TakeOffTime (HHmm)\n",
    "    CAST(\n",
    "        date_format(\n",
    "            CRSDepTimestamp\n",
    "            + (COALESCE(CAST(DepDelay AS INT), 0)\n",
    "            + COALESCE(CAST(TaxiOut AS INT), 0)) * INTERVAL 1 MINUTE,\n",
    "            'HHmm'\n",
    "        ) AS INT\n",
    "    ) AS TakeOffTime,\n",
    "\n",
    "    -- 4. LandingEst (HHmm)\n",
    "    CAST(\n",
    "        date_format(\n",
    "            CRSDepTimestamp\n",
    "            + COALESCE(CAST(CRSElapsedTime AS INT), 0) * INTERVAL 1 MINUTE,\n",
    "            'HHmm'\n",
    "        ) AS INT\n",
    "    ) AS LandingEst\n",
    "\n",
    "FROM __THIS__\n",
    "\"\"\"\n",
    "\n",
    "# Creamos el transformador\n",
    "sql_trans = SQLTransformer(statement=sql_logic)\n",
    "\n",
    "stages.append(sql_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddc67e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_split_sql = \"\"\"\n",
    "SELECT\n",
    "    *,\n",
    "    \n",
    "    -- CRSDepTime\n",
    "    CAST(FLOOR(CRSDepTime / 100) AS INT)      AS CRSDepHour,\n",
    "    CAST(CRSDepTime % 100 AS INT)             AS CRSDepMinute,\n",
    "\n",
    "    -- CRSArrTime\n",
    "    CAST(FLOOR(CRSArrTime / 100) AS INT)      AS CRSArrHour,\n",
    "    CAST(CRSArrTime % 100 AS INT)             AS CRSArrMinute,\n",
    "\n",
    "    -- TakeOffTime\n",
    "    CAST(FLOOR(TakeOffTime / 100) AS INT)     AS TakeOffHour,\n",
    "    CAST(TakeOffTime % 100 AS INT)            AS TakeOffMinute,\n",
    "\n",
    "    -- LandingEst\n",
    "    CAST(FLOOR(LandingEst / 100) AS INT)      AS LandingHour,\n",
    "    CAST(LandingEst % 100 AS INT)             AS LandingMinute\n",
    "\n",
    "FROM __THIS__\n",
    "\"\"\"\n",
    "\n",
    "time_splitter = SQLTransformer(statement=time_split_sql)\n",
    "\n",
    "stages.append(time_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13fa2176",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_hhmm_sql = \"\"\"\n",
    "SELECT\n",
    "    * EXCEPT (\n",
    "        CRSDepTime,\n",
    "        CRSArrTime,\n",
    "        TakeOffTime,\n",
    "        LandingEst\n",
    "    )\n",
    "FROM __THIS__\n",
    "\"\"\"\n",
    "\n",
    "drop_hhmm = SQLTransformer(statement=drop_hhmm_sql)\n",
    "stages.append(drop_hhmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42a7fe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Definición de Variables ---\n",
    "\n",
    "# Selecciona tus 5 variables categóricas (Ejemplos basados en flight data)\n",
    "cat_cols = [\"UniqueCarrier\", \"Origin\", \"Dest\", \"Month\", \"DayOfWeek\", \"DayofMonth\", \"Year\"]\n",
    "\n",
    "# Selecciona tus 10 variables numéricas\n",
    "num_cols = [\n",
    "    \"DepDelay\", \"TaxiOut\", \"Distance\", \"CRSElapsedTime\", \n",
    "    \"LandingEstMinute\", \"TakeOffTimeMinute\", \"CRSDepTimeMinute\", \"CRSArrTimeMinute\", \"DepTimeMinute\",\n",
    "    \"LandingEstHour\", \"TakeOffTimeHour\", \"CRSDepTimeHour\", \"CRSArrTimeHour\", \"DepTimeHour\" #Dividir previamente en hour y minute\n",
    "]\n",
    "\n",
    "\n",
    "# A) Procesamiento de Categóricas (StringIndexer + OHE)\n",
    "input_cols_ohe = []\n",
    "\n",
    "for c in cat_cols:\n",
    "    # 1. Indexar: Convierte strings a índices numéricos\n",
    "    indexer = StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\")\n",
    "    \n",
    "    # 2. OHE: Convierte índices a vectores dispersos (sparse vectors)\n",
    "    encoder = OneHotEncoder(inputCol=f\"{c}_idx\", outputCol=f\"{c}_ohe\")\n",
    "    \n",
    "    # Añadimos pasos al pipeline y guardamos el nombre de la columna de salida\n",
    "    stages += [indexer, encoder]\n",
    "    input_cols_ohe.append(f\"{c}_ohe\")\n",
    "\n",
    "# B) Procesamiento de Numéricas (Assembler + StandardScaler)\n",
    "# Nota: StandardScaler en Spark requiere una columna de tipo Vector, no columnas sueltas.\n",
    "\n",
    "# 1. Agrupar todas las numéricas en un solo vector temporal\n",
    "num_assembler = VectorAssembler(inputCols=num_cols, outputCol=\"num_features_raw\", handleInvalid=\"skip\")\n",
    "stages.append(num_assembler)\n",
    "\n",
    "# 2. Estandarizar ese vector (Media 0, Desviación Estándar 1)\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"num_features_raw\", \n",
    "    outputCol=\"num_features_scaled\", \n",
    "    withStd=True, \n",
    "    withMean=True\n",
    ")\n",
    "stages.append(scaler)\n",
    "\n",
    "# C) Ensamblaje Final (Unir OHE + Numéricas Escaladas)\n",
    "\n",
    "# Juntamos las columnas OHE y la columna de numéricas ya escaladas\n",
    "assembler_all_inputs = input_cols_ohe + [\"num_features_scaled\"]\n",
    "\n",
    "assembler_all = VectorAssembler(\n",
    "    inputCols=assembler_all_inputs, \n",
    "    outputCol=\"features\" # Esta es la columna estándar para modelos ML en Spark\n",
    ")\n",
    "stages.append(assembler_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ad5b74",
   "metadata": {},
   "source": [
    "### Baseline model training\n",
    "\n",
    "Three baseline models are trained: `LogisticRegression(max_iter=1000)`, `DecisionTreeClassifier()`, and `MLPClassifier(max_iter=500)`. These models are fitted on the feature-selected and scaled training data. Training multiple models at this stage establishes performance benchmarks for comparison and identifies which algorithms are initially more suitable for the dataset before any tuning is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c251e966",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T19:53:18.023080Z",
     "start_time": "2025-12-11T19:53:12.054651Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import DecisionTreeRegressor, RandomForestRegressor, GBTRegressor\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# 1. Definición de Modelos\n",
    "dt = DecisionTreeRegressor(labelCol=\"ArrDelay\", featuresCol=\"features\")\n",
    "rf = RandomForestRegressor(labelCol=\"ArrDelay\", featuresCol=\"features\")\n",
    "gbt = GBTRegressor(labelCol=\"ArrDelay\", featuresCol=\"features\")\n",
    "\n",
    "# Creación de Pipelines (usando concatenación para mayor limpieza)\n",
    "pipeline_dt = Pipeline(stages=stages + [dt])\n",
    "pipeline_rf = Pipeline(stages=stages + [rf])\n",
    "pipeline_gbt = Pipeline(stages=stages + [gbt])\n",
    "\n",
    "# 2. Rejillas de parámetros (ParamGrids)\n",
    "# IMPORTANTE: Los parámetros deben coincidir con la instancia del modelo\n",
    "paramGrid_dt = (ParamGridBuilder()\n",
    "    .addGrid(dt.maxDepth, [5])    # Puedes añadir mas valores si tienes el musculo tecnico para hacerlo. Nosotros no lo tenemos - La RAM esta muy cara\n",
    "    .addGrid(dt.maxBins, [32])   # Puedes añadir mas valores si tienes el musculo tecnico para hacerlo. Nosotros no lo tenemos - La RAM esta muy cara\n",
    "    .build())\n",
    "\n",
    "paramGrid_rf = (ParamGridBuilder()\n",
    "    .addGrid(rf.numTrees, [20])   # Puedes añadir mas valores si tienes el musculo tecnico para hacerlo. Nosotros no lo tenemos - La RAM esta muy cara\n",
    "    .addGrid(rf.maxDepth, [5])    # Puedes añadir mas valores si tienes el musculo tecnico para hacerlo. Nosotros no lo tenemos - La RAM esta muy cara\n",
    "    .build())\n",
    "\n",
    "paramGrid_gbt = (ParamGridBuilder()\n",
    "    .addGrid(gbt.maxIter, [10])   # Puedes añadir mas valores si tienes el musculo tecnico para hacerlo. Nosotros no lo tenemos - La RAM esta muy cara\n",
    "    .addGrid(gbt.maxDepth, [3])    # Puedes añadir mas valores si tienes el musculo tecnico para hacerlo. Nosotros no lo tenemos - La RAM esta muy cara\n",
    "    .build())\n",
    "# 3. CrossValidators\n",
    "# Configuramos el evaluador una sola vez para reutilizarlo\n",
    "evaluador = RegressionEvaluator(labelCol=\"ArrDelay\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "cv_dt = CrossValidator(\n",
    "    estimator=pipeline_dt, \n",
    "    estimatorParamMaps=paramGrid_dt,\n",
    "    evaluator=evaluador,\n",
    "    numFolds=3,\n",
    "    seed=89,\n",
    "    parallelism=1\n",
    ")\n",
    "\n",
    "cv_rf = CrossValidator(\n",
    "    estimator=pipeline_rf,\n",
    "    estimatorParamMaps=paramGrid_rf,\n",
    "    evaluator=evaluador,\n",
    "    numFolds=3,\n",
    "    seed=89,\n",
    "    parallelism=1\n",
    ")\n",
    "\n",
    "cv_gbt = CrossValidator(\n",
    "    estimator=pipeline_gbt,\n",
    "    estimatorParamMaps=paramGrid_gbt,\n",
    "    evaluator=evaluador,\n",
    "    numFolds=3,\n",
    "    seed=89,\n",
    "    parallelism=1\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ead558",
   "metadata": {},
   "source": [
    "### Baseline evaluation\n",
    "\n",
    "The `evaluate_full()` function is used to evaluate each model on the test set. It calculates standard metrics including accuracy, precision, recall, and F1-score. It also plots a confusion matrix, the ROC curve with AUC, and the precision-recall curve with AUC-PR. These metrics provide a comprehensive overview of each model's performance and allow for a more nuanced understanding of strengths and weaknesses, especially in cases of class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b190e246",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T19:53:18.705094Z",
     "start_time": "2025-12-11T19:53:18.032714Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Cross-Validation para Decision Tree...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[AMBIGUOUS_COLUMN_OR_FIELD] Column or field `TaxiOut` is ambiguous and has 2 matches. SQLSTATE: 42702",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgc\u001b[39;00m\n\u001b[1;32m     15\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m---> 16\u001b[0m cv_model \u001b[38;5;241m=\u001b[39m \u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# 2. EVALUACIÓN (Validación con datos no vistos)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Usamos el cv_model (que ya contiene el mejor modelo interno) para predecir\u001b[39;00m\n\u001b[1;32m     20\u001b[0m predicciones \u001b[38;5;241m=\u001b[39m cv_model\u001b[38;5;241m.\u001b[39mtransform(test_data)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:203\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    208\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/tuning.py:858\u001b[0m, in \u001b[0;36mCrossValidator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    852\u001b[0m train \u001b[38;5;241m=\u001b[39m datasets[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m    854\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    855\u001b[0m     inheritable_thread_target(dataset\u001b[38;5;241m.\u001b[39msparkSession),\n\u001b[1;32m    856\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    857\u001b[0m )\n\u001b[0;32m--> 858\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: f(), tasks):\n\u001b[1;32m    859\u001b[0m     metrics_all[i][j] \u001b[38;5;241m=\u001b[39m metric\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:873\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 873\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    123\u001b[0m job, i, func, args, kwds \u001b[38;5;241m=\u001b[39m task\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrap_exception \u001b[38;5;129;01mand\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _helper_reraises_exception:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/tuning.py:858\u001b[0m, in \u001b[0;36mCrossValidator._fit.<locals>.<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    852\u001b[0m train \u001b[38;5;241m=\u001b[39m datasets[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m    854\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    855\u001b[0m     inheritable_thread_target(dataset\u001b[38;5;241m.\u001b[39msparkSession),\n\u001b[1;32m    856\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    857\u001b[0m )\n\u001b[0;32m--> 858\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, tasks):\n\u001b[1;32m    859\u001b[0m     metrics_all[i][j] \u001b[38;5;241m=\u001b[39m metric\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/util.py:435\u001b[0m, in \u001b[0;36minheritable_thread_target.<locals>.outer.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m tags:\n\u001b[1;32m    434\u001b[0m     session\u001b[38;5;241m.\u001b[39maddTag(tag)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 435\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mff\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/tuning.py:115\u001b[0m, in \u001b[0;36m_parallelFitTasks.<locals>.singleTask\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msingleTask\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, Transformer]:\n\u001b[0;32m--> 115\u001b[0m     index, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodelIter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# TODO: duplicate evaluator to take extra params from input\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m#  Note: Supporting tuning params in evaluator need update method\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;66;03m#  all nested stages and evaluators\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     metric \u001b[38;5;241m=\u001b[39m eva\u001b[38;5;241m.\u001b[39mevaluate(model\u001b[38;5;241m.\u001b[39mtransform(validation, epm[index]))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:96\u001b[0m, in \u001b[0;36m_FitMultipleIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo models remaining.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcounter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfitSingleModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:154\u001b[0m, in \u001b[0;36mEstimator.fitMultiple.<locals>.fitSingleModel\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfitSingleModel\u001b[39m(index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m M:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparamMaps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:201\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(params, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m params:\n\u001b[0;32m--> 201\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/pipeline.py:136\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stage, Transformer):\n\u001b[1;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(stage)\n\u001b[0;32m--> 136\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     model \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mfit(dataset)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:260\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_transform(dataset)\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/util.py:212\u001b[0m, in \u001b[0;36mtry_remote_transform_relation.<locals>.wrapped\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:429\u001b[0m, in \u001b[0;36mJavaTransformer._transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m, dataset\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    284\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [AMBIGUOUS_COLUMN_OR_FIELD] Column or field `TaxiOut` is ambiguous and has 2 matches. SQLSTATE: 42702"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "modelos_cv = [(\"Decision Tree\", cv_dt), (\"Random Forest\", cv_rf), (\"GBT\", cv_gbt)]\n",
    "resultados = []\n",
    "\n",
    "# Evaluadores\n",
    "eval_mae = RegressionEvaluator(labelCol=\"ArrDelay\", metricName=\"mae\")\n",
    "# 'evaluador' ya lo tienes definido arriba como RMSE\n",
    "\n",
    "for nombre, cv in modelos_cv:\n",
    "    print(f\"Iniciando Cross-Validation para {nombre}...\")\n",
    "    \n",
    "    # 1. AJUSTE DE HIPERPARÁMETROS (Tuning)\n",
    "    # Aquí Spark entrena los 'n' folds y selecciona la mejor combinación de la rejilla\n",
    "    spark.catalog.clearCache()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    cv_model = cv.fit(train_data) \n",
    "    \n",
    "    # 2. EVALUACIÓN (Validación con datos no vistos)\n",
    "    # Usamos el cv_model (que ya contiene el mejor modelo interno) para predecir\n",
    "    predicciones = cv_model.transform(test_data)\n",
    "    \n",
    "    rmse = evaluador.evaluate(predicciones) # RMSE\n",
    "    mae = eval_mae.evaluate(predicciones)   # MAE\n",
    "    \n",
    "    # Tu lógica de decisión: Si RMSE es muy alto respecto al MAE, priorizar MAE\n",
    "    score = mae if rmse > (2 * mae) else rmse\n",
    "    metrica_usada = \"MAE\" if rmse > (2 * mae) else \"RMSE\"\n",
    "    \n",
    "    print(f\"Resultados {nombre} -> RMSE: {rmse:.2f}, MAE: {mae:.2f} (Score: {score:.2f} vía {metrica_usada})\")\n",
    "    \n",
    "    # Guardamos todo el objeto cv_model para poder extraer el mejor modelo después\n",
    "    resultados.append({\n",
    "        \"nombre\": nombre,\n",
    "        \"modelo_fit\": cv_model, \n",
    "        \"rmse\": rmse,\n",
    "        \"mae\": mae,\n",
    "        \"score_final\": score,\n",
    "        \"metrica\": metrica_usada\n",
    "    })\n",
    "\n",
    "# --- PASO D: ENCONTRAR Y GUARDAR EL MEJOR ---\n",
    "\n",
    "# Encontrar el mejor resultado basado en tu score_final\n",
    "mejor_resultado = min(resultados, key=lambda x: x[\"score_final\"])\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"GANADOR: {mejor_resultado['nombre']}\")\n",
    "print(f\"Criterio: {mejor_resultado['metrica']} de {mejor_resultado['score_final']:.4f}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Extraer el modelo definitivo (el que mejor funcionó en el CV)\n",
    "# Nota: cv_model.bestModel devuelve el PipelineModel con los mejores parámetros\n",
    "mejor_modelo_final = mejor_resultado['modelo_fit'].bestModel\n",
    "\n",
    "# 3. ALMACENAMIENTO DEL MODELO (Requisito del proyecto)\n",
    "path_guardado = \"best_model\"\n",
    "mejor_modelo_final.write().overwrite().save(path_guardado)\n",
    "\n",
    "print(f\"Éxito: El modelo '{mejor_resultado['nombre']}' ha sido guardado en la carpeta '{path_guardado}'\")\n",
    "\n",
    "'''modelos_cv = [(\"Decision Tree\", cv_dt), (\"Random Forest\", cv_rf), (\"GBT\", cv_gbt)]\n",
    "resultados = []\n",
    "\n",
    "eval_mae = RegressionEvaluator(labelCol=\"ArrDelay\", metricName=\"mae\")\n",
    "\n",
    "for nombre, cv in modelos_cv:\n",
    "    print(f\"Entrenando {nombre}...\")\n",
    "    fit_model = cv.fit(train_data) # train_data debe estar definido previamente\n",
    "    predicciones = fit_model.transform(test_data)\n",
    "    \n",
    "    rmse = evaluador.evaluate(predicciones) # RMSE\n",
    "    mae = eval_mae.evaluate(predicciones)   # MAE\n",
    "    \n",
    "    # Tu lógica: Si RMSE > 2*MAE, fijarse en MAE, si no en RMSE\n",
    "    score = mae if rmse > (2 * mae) else rmse\n",
    "    metrica_usada = \"MAE\" if rmse > (2 * mae) else \"RMSE\"\n",
    "    \n",
    "    resultados.append({\n",
    "        \"nombre\": nombre,\n",
    "        \"rmse\": rmse,\n",
    "        \"mae\": mae,\n",
    "        \"score_final\": score,\n",
    "        \"metrica\": metrica_usada\n",
    "    })\n",
    "\n",
    "# Encontrar el mejor\n",
    "mejor_modelo = min(resultados, key=lambda x: x[\"score_final\"])\n",
    "print(f\"\\nEl mejor modelo es {mejor_modelo['nombre']} basado en {mejor_modelo['metrica']}\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ed84ff",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "\n",
    "GridSearchCV is used for hyperparameter optimization for each model: logistic regression (`C`, `penalty`, `solver`), decision tree (`max_depth`, `min_samples_split`), and neural network (`hidden_layer_sizes`, `alpha`). The code searches across multiple parameter combinations with 5-fold cross-validation to select the configuration that maximizes accuracy. This step is essential for improving model generalization and ensuring that the final models perform optimally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f08e961",
   "metadata": {},
   "source": [
    "### Final evaluation\n",
    "\n",
    "After tuning, the code retrieves the best model via `grid.best_estimator_` and evaluates it on the test set with `accuracy_score` and `classification_report` again. This step confirms how much the optimized model improves over the baseline. Comparing results allows the user to quantify performance gains resulting from hyperparameter optimization and feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce1c15d",
   "metadata": {},
   "source": [
    "### Interpretability\n",
    "\n",
    "For interpretability, the logistic regression coefficients (betas) are extracted from `best_lr.coef_` and plotted as a horizontal bar chart to visualize the influence of each feature. The decision tree is visualized using `plot_tree()` to show the full branching structure, feature thresholds, Gini impurity, and leaf outcomes. These visualizations help understand how the models make predictions and identify the most important features driving classification decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f847bc63",
   "metadata": {},
   "source": [
    "### Comparison Before vs After Tuning\n",
    "\n",
    "This section summarizes the key metrics (accuracy, precision, recall, F1-score) for all three models in a pandas DataFrame. It compares baseline and tuned models side by side, allowing a clear overview of the improvements achieved through feature selection and hyperparameter tuning. This comparative analysis helps identify which model benefits the most from tuning and provides actionable insights for model selection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
